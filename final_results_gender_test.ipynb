{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from matplotlib.pyplot import specgram\n",
    "import keras\n",
    "from tensorflow.keras import optimizers\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Input, Flatten, Dropout, Activation\n",
    "from keras.layers import Conv1D, MaxPooling1D, AveragePooling1D\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "folders = os.listdir('C:/emotions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(folders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Actor_01', 'Actor_02', 'Actor_03', 'Actor_04', 'Actor_05', 'Actor_06', 'Actor_07', 'Actor_08', 'Actor_09', 'Actor_10', 'Actor_11', 'Actor_12', 'Actor_13', 'Actor_14', 'Actor_15', 'Actor_16', 'Actor_17', 'Actor_18', 'Actor_19', 'Actor_20', 'Actor_21', 'Actor_22', 'Actor_23', 'Actor_24', 'RawData']\n"
     ]
    }
   ],
   "source": [
    "print(folders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Actor_01', 'Actor_02', 'Actor_03', 'Actor_04', 'Actor_05', 'Actor_06', 'Actor_07', 'Actor_08', 'Actor_09', 'Actor_10', 'Actor_11', 'Actor_12', 'Actor_13', 'Actor_14', 'Actor_15', 'Actor_16', 'Actor_17', 'Actor_18', 'Actor_19', 'Actor_20', 'Actor_21', 'Actor_22', 'Actor_23', 'Actor_24']\n"
     ]
    }
   ],
   "source": [
    "print(folders[0:24])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawFolder = folders[24]\n",
    "rawAudioFiles = os.listdir('C:/emotions/' + rawFolder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1920\n"
     ]
    }
   ],
   "source": [
    "print(len(rawAudioFiles))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the audio file's waveform and its spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import glob "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import scipy.io.wavfile\n",
    "import numpy as np\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feeling_list=[]\n",
    "for item in rawAudioFiles:\n",
    "    if item[6:-16]=='02' and int(item[18:-4])%2==0:\n",
    "        feeling_list.append('female_calm')\n",
    "    elif item[6:-16]=='02' and int(item[18:-4])%2==1:\n",
    "        feeling_list.append('male_calm')\n",
    "    elif item[6:-16]=='03' and int(item[18:-4])%2==0:\n",
    "        feeling_list.append('female_happy')\n",
    "    elif item[6:-16]=='03' and int(item[18:-4])%2==1:\n",
    "        feeling_list.append('male_happy')\n",
    "    elif item[6:-16]=='04' and int(item[18:-4])%2==0:\n",
    "        feeling_list.append('female_sad')\n",
    "    elif item[6:-16]=='04' and int(item[18:-4])%2==1:\n",
    "        feeling_list.append('male_sad')\n",
    "    elif item[6:-16]=='05' and int(item[18:-4])%2==0:\n",
    "        feeling_list.append('female_angry')\n",
    "    elif item[6:-16]=='05' and int(item[18:-4])%2==1:\n",
    "        feeling_list.append('male_angry')\n",
    "    elif item[6:-16]=='06' and int(item[18:-4])%2==0:\n",
    "        feeling_list.append('female_fearful')\n",
    "    elif item[6:-16]=='06' and int(item[18:-4])%2==1:\n",
    "        feeling_list.append('male_fearful')\n",
    "    elif item[:1]=='a':\n",
    "        feeling_list.append('male_angry')\n",
    "    elif item[:1]=='f':\n",
    "        feeling_list.append('male_fearful')\n",
    "    elif item[:1]=='h':\n",
    "        feeling_list.append('male_happy')\n",
    "    #elif item[:1]=='n':\n",
    "        #feeling_list.append('neutral')\n",
    "    elif item[:2]=='sa':\n",
    "        feeling_list.append('male_sad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = pd.DataFrame(feeling_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>male_calm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>female_calm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>male_calm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>female_calm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>male_calm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>female_calm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>male_calm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>female_calm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>male_calm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>female_calm</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             0\n",
       "0    male_calm\n",
       "1  female_calm\n",
       "2    male_calm\n",
       "3  female_calm\n",
       "4    male_calm\n",
       "5  female_calm\n",
       "6    male_calm\n",
       "7  female_calm\n",
       "8    male_calm\n",
       "9  female_calm"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1920"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rawAudioFiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the features of audio files using librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['feature'])\n",
    "bookmark=0\n",
    "for index,y in enumerate(rawAudioFiles):\n",
    "    if rawAudioFiles[index][6:-16]!='01' and rawAudioFiles[index][6:-16]!='07' and rawAudioFiles[index][6:-16]!='08' and rawAudioFiles[index][:2]!='su' and rawAudioFiles[index][:1]!='n' and rawAudioFiles[index][:1]!='d':\n",
    "        X, sample_rate = librosa.load('C:/emotions/RawData/'+ y, res_type='kaiser_fast',duration=2.5,sr=22050*2,offset=0.5)\n",
    "        sample_rate = np.array(sample_rate)\n",
    "        mfccs = np.mean(librosa.feature.mfcc(y=X, \n",
    "                                            sr=sample_rate, \n",
    "                                            n_mfcc=13),\n",
    "                        axis=0)\n",
    "        feature = mfccs\n",
    "        #[float(i) for i in feature]\n",
    "        #feature1=feature[:135]\n",
    "        df.loc[bookmark] = [feature]\n",
    "        bookmark=bookmark+1        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1200"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df3 = pd.DataFrame(df['feature'].values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>206</th>\n",
       "      <th>207</th>\n",
       "      <th>208</th>\n",
       "      <th>209</th>\n",
       "      <th>210</th>\n",
       "      <th>211</th>\n",
       "      <th>212</th>\n",
       "      <th>213</th>\n",
       "      <th>214</th>\n",
       "      <th>215</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-70.267769</td>\n",
       "      <td>-70.267769</td>\n",
       "      <td>-70.267769</td>\n",
       "      <td>-70.267769</td>\n",
       "      <td>-70.267769</td>\n",
       "      <td>-70.267769</td>\n",
       "      <td>-70.267769</td>\n",
       "      <td>-70.267769</td>\n",
       "      <td>-70.267769</td>\n",
       "      <td>-70.267769</td>\n",
       "      <td>...</td>\n",
       "      <td>-58.802044</td>\n",
       "      <td>-57.447464</td>\n",
       "      <td>-58.896500</td>\n",
       "      <td>-58.750996</td>\n",
       "      <td>-57.405678</td>\n",
       "      <td>-60.078484</td>\n",
       "      <td>-63.426800</td>\n",
       "      <td>-62.638542</td>\n",
       "      <td>-61.082737</td>\n",
       "      <td>-60.234661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-65.707649</td>\n",
       "      <td>-65.707649</td>\n",
       "      <td>-63.114719</td>\n",
       "      <td>-61.518997</td>\n",
       "      <td>-61.097141</td>\n",
       "      <td>-63.424599</td>\n",
       "      <td>-63.720066</td>\n",
       "      <td>-56.854614</td>\n",
       "      <td>-55.168972</td>\n",
       "      <td>-54.639999</td>\n",
       "      <td>...</td>\n",
       "      <td>-38.301201</td>\n",
       "      <td>-39.792141</td>\n",
       "      <td>-40.613159</td>\n",
       "      <td>-41.209202</td>\n",
       "      <td>-41.439201</td>\n",
       "      <td>-43.994278</td>\n",
       "      <td>-49.399620</td>\n",
       "      <td>-50.591599</td>\n",
       "      <td>-49.144051</td>\n",
       "      <td>-48.705654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-65.482498</td>\n",
       "      <td>-65.482498</td>\n",
       "      <td>-65.482498</td>\n",
       "      <td>-65.482498</td>\n",
       "      <td>-65.482498</td>\n",
       "      <td>-65.482498</td>\n",
       "      <td>-65.482498</td>\n",
       "      <td>-65.482498</td>\n",
       "      <td>-65.482498</td>\n",
       "      <td>-65.482498</td>\n",
       "      <td>...</td>\n",
       "      <td>-29.193260</td>\n",
       "      <td>-31.346556</td>\n",
       "      <td>-34.310772</td>\n",
       "      <td>-35.800705</td>\n",
       "      <td>-35.936115</td>\n",
       "      <td>-37.631844</td>\n",
       "      <td>-40.119411</td>\n",
       "      <td>-41.662888</td>\n",
       "      <td>-41.323643</td>\n",
       "      <td>-40.710770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-64.528450</td>\n",
       "      <td>-64.528450</td>\n",
       "      <td>-64.528450</td>\n",
       "      <td>-64.528450</td>\n",
       "      <td>-64.528450</td>\n",
       "      <td>-64.528450</td>\n",
       "      <td>-64.528450</td>\n",
       "      <td>-64.528450</td>\n",
       "      <td>-64.528450</td>\n",
       "      <td>-65.928223</td>\n",
       "      <td>...</td>\n",
       "      <td>-48.872002</td>\n",
       "      <td>-48.674301</td>\n",
       "      <td>-48.596073</td>\n",
       "      <td>-47.602745</td>\n",
       "      <td>-43.049198</td>\n",
       "      <td>-42.659542</td>\n",
       "      <td>-43.188560</td>\n",
       "      <td>-44.001244</td>\n",
       "      <td>-43.610100</td>\n",
       "      <td>-44.698246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-62.364311</td>\n",
       "      <td>-59.934727</td>\n",
       "      <td>-61.869602</td>\n",
       "      <td>-67.495773</td>\n",
       "      <td>-71.071808</td>\n",
       "      <td>-65.679817</td>\n",
       "      <td>-63.394402</td>\n",
       "      <td>-65.503349</td>\n",
       "      <td>-61.856644</td>\n",
       "      <td>-60.005428</td>\n",
       "      <td>...</td>\n",
       "      <td>-33.817356</td>\n",
       "      <td>-39.071327</td>\n",
       "      <td>-41.897121</td>\n",
       "      <td>-40.865437</td>\n",
       "      <td>-38.290604</td>\n",
       "      <td>-36.372398</td>\n",
       "      <td>-37.915779</td>\n",
       "      <td>-40.026127</td>\n",
       "      <td>-43.383774</td>\n",
       "      <td>-43.965401</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 216 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0          1          2          3          4          5    \\\n",
       "0 -70.267769 -70.267769 -70.267769 -70.267769 -70.267769 -70.267769   \n",
       "1 -65.707649 -65.707649 -63.114719 -61.518997 -61.097141 -63.424599   \n",
       "2 -65.482498 -65.482498 -65.482498 -65.482498 -65.482498 -65.482498   \n",
       "3 -64.528450 -64.528450 -64.528450 -64.528450 -64.528450 -64.528450   \n",
       "4 -62.364311 -59.934727 -61.869602 -67.495773 -71.071808 -65.679817   \n",
       "\n",
       "         6          7          8          9    ...        206        207  \\\n",
       "0 -70.267769 -70.267769 -70.267769 -70.267769  ... -58.802044 -57.447464   \n",
       "1 -63.720066 -56.854614 -55.168972 -54.639999  ... -38.301201 -39.792141   \n",
       "2 -65.482498 -65.482498 -65.482498 -65.482498  ... -29.193260 -31.346556   \n",
       "3 -64.528450 -64.528450 -64.528450 -65.928223  ... -48.872002 -48.674301   \n",
       "4 -63.394402 -65.503349 -61.856644 -60.005428  ... -33.817356 -39.071327   \n",
       "\n",
       "         208        209        210        211        212        213  \\\n",
       "0 -58.896500 -58.750996 -57.405678 -60.078484 -63.426800 -62.638542   \n",
       "1 -40.613159 -41.209202 -41.439201 -43.994278 -49.399620 -50.591599   \n",
       "2 -34.310772 -35.800705 -35.936115 -37.631844 -40.119411 -41.662888   \n",
       "3 -48.596073 -47.602745 -43.049198 -42.659542 -43.188560 -44.001244   \n",
       "4 -41.897121 -40.865437 -38.290604 -36.372398 -37.915779 -40.026127   \n",
       "\n",
       "         214        215  \n",
       "0 -61.082737 -60.234661  \n",
       "1 -49.144051 -48.705654  \n",
       "2 -41.323643 -40.710770  \n",
       "3 -43.610100 -44.698246  \n",
       "4 -43.383774 -43.965401  \n",
       "\n",
       "[5 rows x 216 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newdf = pd.concat([df3,labels], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rnewdf = newdf.rename(index=str, columns={\"0\": \"label\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>207</th>\n",
       "      <th>208</th>\n",
       "      <th>209</th>\n",
       "      <th>210</th>\n",
       "      <th>211</th>\n",
       "      <th>212</th>\n",
       "      <th>213</th>\n",
       "      <th>214</th>\n",
       "      <th>215</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-70.267769</td>\n",
       "      <td>-70.267769</td>\n",
       "      <td>-70.267769</td>\n",
       "      <td>-70.267769</td>\n",
       "      <td>-70.267769</td>\n",
       "      <td>-70.267769</td>\n",
       "      <td>-70.267769</td>\n",
       "      <td>-70.267769</td>\n",
       "      <td>-70.267769</td>\n",
       "      <td>-70.267769</td>\n",
       "      <td>...</td>\n",
       "      <td>-57.447464</td>\n",
       "      <td>-58.896500</td>\n",
       "      <td>-58.750996</td>\n",
       "      <td>-57.405678</td>\n",
       "      <td>-60.078484</td>\n",
       "      <td>-63.426800</td>\n",
       "      <td>-62.638542</td>\n",
       "      <td>-61.082737</td>\n",
       "      <td>-60.234661</td>\n",
       "      <td>male_calm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-65.707649</td>\n",
       "      <td>-65.707649</td>\n",
       "      <td>-63.114719</td>\n",
       "      <td>-61.518997</td>\n",
       "      <td>-61.097141</td>\n",
       "      <td>-63.424599</td>\n",
       "      <td>-63.720066</td>\n",
       "      <td>-56.854614</td>\n",
       "      <td>-55.168972</td>\n",
       "      <td>-54.639999</td>\n",
       "      <td>...</td>\n",
       "      <td>-39.792141</td>\n",
       "      <td>-40.613159</td>\n",
       "      <td>-41.209202</td>\n",
       "      <td>-41.439201</td>\n",
       "      <td>-43.994278</td>\n",
       "      <td>-49.399620</td>\n",
       "      <td>-50.591599</td>\n",
       "      <td>-49.144051</td>\n",
       "      <td>-48.705654</td>\n",
       "      <td>female_calm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-65.482498</td>\n",
       "      <td>-65.482498</td>\n",
       "      <td>-65.482498</td>\n",
       "      <td>-65.482498</td>\n",
       "      <td>-65.482498</td>\n",
       "      <td>-65.482498</td>\n",
       "      <td>-65.482498</td>\n",
       "      <td>-65.482498</td>\n",
       "      <td>-65.482498</td>\n",
       "      <td>-65.482498</td>\n",
       "      <td>...</td>\n",
       "      <td>-31.346556</td>\n",
       "      <td>-34.310772</td>\n",
       "      <td>-35.800705</td>\n",
       "      <td>-35.936115</td>\n",
       "      <td>-37.631844</td>\n",
       "      <td>-40.119411</td>\n",
       "      <td>-41.662888</td>\n",
       "      <td>-41.323643</td>\n",
       "      <td>-40.710770</td>\n",
       "      <td>male_calm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-64.528450</td>\n",
       "      <td>-64.528450</td>\n",
       "      <td>-64.528450</td>\n",
       "      <td>-64.528450</td>\n",
       "      <td>-64.528450</td>\n",
       "      <td>-64.528450</td>\n",
       "      <td>-64.528450</td>\n",
       "      <td>-64.528450</td>\n",
       "      <td>-64.528450</td>\n",
       "      <td>-65.928223</td>\n",
       "      <td>...</td>\n",
       "      <td>-48.674301</td>\n",
       "      <td>-48.596073</td>\n",
       "      <td>-47.602745</td>\n",
       "      <td>-43.049198</td>\n",
       "      <td>-42.659542</td>\n",
       "      <td>-43.188560</td>\n",
       "      <td>-44.001244</td>\n",
       "      <td>-43.610100</td>\n",
       "      <td>-44.698246</td>\n",
       "      <td>female_calm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-62.364311</td>\n",
       "      <td>-59.934727</td>\n",
       "      <td>-61.869602</td>\n",
       "      <td>-67.495773</td>\n",
       "      <td>-71.071808</td>\n",
       "      <td>-65.679817</td>\n",
       "      <td>-63.394402</td>\n",
       "      <td>-65.503349</td>\n",
       "      <td>-61.856644</td>\n",
       "      <td>-60.005428</td>\n",
       "      <td>...</td>\n",
       "      <td>-39.071327</td>\n",
       "      <td>-41.897121</td>\n",
       "      <td>-40.865437</td>\n",
       "      <td>-38.290604</td>\n",
       "      <td>-36.372398</td>\n",
       "      <td>-37.915779</td>\n",
       "      <td>-40.026127</td>\n",
       "      <td>-43.383774</td>\n",
       "      <td>-43.965401</td>\n",
       "      <td>male_calm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1195</th>\n",
       "      <td>-24.313856</td>\n",
       "      <td>-24.041178</td>\n",
       "      <td>-25.228422</td>\n",
       "      <td>-26.094988</td>\n",
       "      <td>-24.132362</td>\n",
       "      <td>-25.118191</td>\n",
       "      <td>-25.258566</td>\n",
       "      <td>-24.432186</td>\n",
       "      <td>-24.738892</td>\n",
       "      <td>-24.688087</td>\n",
       "      <td>...</td>\n",
       "      <td>-8.436266</td>\n",
       "      <td>-10.197830</td>\n",
       "      <td>-12.263414</td>\n",
       "      <td>-14.144474</td>\n",
       "      <td>-7.916656</td>\n",
       "      <td>-3.283535</td>\n",
       "      <td>-2.470568</td>\n",
       "      <td>-2.487463</td>\n",
       "      <td>-1.785601</td>\n",
       "      <td>male_sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1196</th>\n",
       "      <td>-27.733097</td>\n",
       "      <td>-25.317772</td>\n",
       "      <td>-24.269955</td>\n",
       "      <td>-25.217028</td>\n",
       "      <td>-24.375731</td>\n",
       "      <td>-23.234041</td>\n",
       "      <td>-23.060852</td>\n",
       "      <td>-23.593578</td>\n",
       "      <td>-25.465048</td>\n",
       "      <td>-24.390287</td>\n",
       "      <td>...</td>\n",
       "      <td>-19.613743</td>\n",
       "      <td>-22.958599</td>\n",
       "      <td>-22.451134</td>\n",
       "      <td>-22.969593</td>\n",
       "      <td>-24.166462</td>\n",
       "      <td>-23.357229</td>\n",
       "      <td>-23.805897</td>\n",
       "      <td>-22.046549</td>\n",
       "      <td>-21.264595</td>\n",
       "      <td>male_sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1197</th>\n",
       "      <td>-26.477873</td>\n",
       "      <td>-21.849876</td>\n",
       "      <td>-11.200557</td>\n",
       "      <td>-6.713976</td>\n",
       "      <td>-5.940035</td>\n",
       "      <td>-6.923090</td>\n",
       "      <td>-7.600639</td>\n",
       "      <td>-7.370038</td>\n",
       "      <td>-7.653201</td>\n",
       "      <td>-9.667166</td>\n",
       "      <td>...</td>\n",
       "      <td>-7.755277</td>\n",
       "      <td>-6.800155</td>\n",
       "      <td>-6.931998</td>\n",
       "      <td>-8.070164</td>\n",
       "      <td>-6.287150</td>\n",
       "      <td>-5.732646</td>\n",
       "      <td>-5.968968</td>\n",
       "      <td>-2.897266</td>\n",
       "      <td>-0.117553</td>\n",
       "      <td>male_sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1198</th>\n",
       "      <td>-28.242472</td>\n",
       "      <td>-26.161549</td>\n",
       "      <td>-24.954851</td>\n",
       "      <td>-23.735605</td>\n",
       "      <td>-26.121582</td>\n",
       "      <td>-25.552244</td>\n",
       "      <td>-22.299961</td>\n",
       "      <td>-8.970101</td>\n",
       "      <td>-3.758892</td>\n",
       "      <td>-3.458179</td>\n",
       "      <td>...</td>\n",
       "      <td>-25.844707</td>\n",
       "      <td>-25.865776</td>\n",
       "      <td>-22.196074</td>\n",
       "      <td>-22.867290</td>\n",
       "      <td>-23.568983</td>\n",
       "      <td>-23.386150</td>\n",
       "      <td>-24.064566</td>\n",
       "      <td>-22.987600</td>\n",
       "      <td>-23.478584</td>\n",
       "      <td>male_sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1199</th>\n",
       "      <td>-23.072609</td>\n",
       "      <td>-23.384209</td>\n",
       "      <td>-24.084469</td>\n",
       "      <td>-23.670692</td>\n",
       "      <td>-24.399849</td>\n",
       "      <td>-23.972446</td>\n",
       "      <td>-24.582552</td>\n",
       "      <td>-26.315779</td>\n",
       "      <td>-24.173475</td>\n",
       "      <td>-23.067106</td>\n",
       "      <td>...</td>\n",
       "      <td>-22.459688</td>\n",
       "      <td>-22.132027</td>\n",
       "      <td>-21.918674</td>\n",
       "      <td>-22.807762</td>\n",
       "      <td>-22.164743</td>\n",
       "      <td>-21.467731</td>\n",
       "      <td>-20.065800</td>\n",
       "      <td>-21.983009</td>\n",
       "      <td>-25.672873</td>\n",
       "      <td>male_sad</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1200 rows × 217 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0          1          2          3          4          5    \\\n",
       "0    -70.267769 -70.267769 -70.267769 -70.267769 -70.267769 -70.267769   \n",
       "1    -65.707649 -65.707649 -63.114719 -61.518997 -61.097141 -63.424599   \n",
       "2    -65.482498 -65.482498 -65.482498 -65.482498 -65.482498 -65.482498   \n",
       "3    -64.528450 -64.528450 -64.528450 -64.528450 -64.528450 -64.528450   \n",
       "4    -62.364311 -59.934727 -61.869602 -67.495773 -71.071808 -65.679817   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "1195 -24.313856 -24.041178 -25.228422 -26.094988 -24.132362 -25.118191   \n",
       "1196 -27.733097 -25.317772 -24.269955 -25.217028 -24.375731 -23.234041   \n",
       "1197 -26.477873 -21.849876 -11.200557  -6.713976  -5.940035  -6.923090   \n",
       "1198 -28.242472 -26.161549 -24.954851 -23.735605 -26.121582 -25.552244   \n",
       "1199 -23.072609 -23.384209 -24.084469 -23.670692 -24.399849 -23.972446   \n",
       "\n",
       "            6          7          8          9    ...        207        208  \\\n",
       "0    -70.267769 -70.267769 -70.267769 -70.267769  ... -57.447464 -58.896500   \n",
       "1    -63.720066 -56.854614 -55.168972 -54.639999  ... -39.792141 -40.613159   \n",
       "2    -65.482498 -65.482498 -65.482498 -65.482498  ... -31.346556 -34.310772   \n",
       "3    -64.528450 -64.528450 -64.528450 -65.928223  ... -48.674301 -48.596073   \n",
       "4    -63.394402 -65.503349 -61.856644 -60.005428  ... -39.071327 -41.897121   \n",
       "...         ...        ...        ...        ...  ...        ...        ...   \n",
       "1195 -25.258566 -24.432186 -24.738892 -24.688087  ...  -8.436266 -10.197830   \n",
       "1196 -23.060852 -23.593578 -25.465048 -24.390287  ... -19.613743 -22.958599   \n",
       "1197  -7.600639  -7.370038  -7.653201  -9.667166  ...  -7.755277  -6.800155   \n",
       "1198 -22.299961  -8.970101  -3.758892  -3.458179  ... -25.844707 -25.865776   \n",
       "1199 -24.582552 -26.315779 -24.173475 -23.067106  ... -22.459688 -22.132027   \n",
       "\n",
       "            209        210        211        212        213        214  \\\n",
       "0    -58.750996 -57.405678 -60.078484 -63.426800 -62.638542 -61.082737   \n",
       "1    -41.209202 -41.439201 -43.994278 -49.399620 -50.591599 -49.144051   \n",
       "2    -35.800705 -35.936115 -37.631844 -40.119411 -41.662888 -41.323643   \n",
       "3    -47.602745 -43.049198 -42.659542 -43.188560 -44.001244 -43.610100   \n",
       "4    -40.865437 -38.290604 -36.372398 -37.915779 -40.026127 -43.383774   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "1195 -12.263414 -14.144474  -7.916656  -3.283535  -2.470568  -2.487463   \n",
       "1196 -22.451134 -22.969593 -24.166462 -23.357229 -23.805897 -22.046549   \n",
       "1197  -6.931998  -8.070164  -6.287150  -5.732646  -5.968968  -2.897266   \n",
       "1198 -22.196074 -22.867290 -23.568983 -23.386150 -24.064566 -22.987600   \n",
       "1199 -21.918674 -22.807762 -22.164743 -21.467731 -20.065800 -21.983009   \n",
       "\n",
       "            215          0    \n",
       "0    -60.234661    male_calm  \n",
       "1    -48.705654  female_calm  \n",
       "2    -40.710770    male_calm  \n",
       "3    -44.698246  female_calm  \n",
       "4    -43.965401    male_calm  \n",
       "...         ...          ...  \n",
       "1195  -1.785601     male_sad  \n",
       "1196 -21.264595     male_sad  \n",
       "1197  -0.117553     male_sad  \n",
       "1198 -23.478584     male_sad  \n",
       "1199 -25.672873     male_sad  \n",
       "\n",
       "[1200 rows x 217 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnewdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>207</th>\n",
       "      <th>208</th>\n",
       "      <th>209</th>\n",
       "      <th>210</th>\n",
       "      <th>211</th>\n",
       "      <th>212</th>\n",
       "      <th>213</th>\n",
       "      <th>214</th>\n",
       "      <th>215</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>-66.471031</td>\n",
       "      <td>-66.464798</td>\n",
       "      <td>-66.471031</td>\n",
       "      <td>-66.471031</td>\n",
       "      <td>-66.471031</td>\n",
       "      <td>-66.471031</td>\n",
       "      <td>-66.471031</td>\n",
       "      <td>-66.471031</td>\n",
       "      <td>-66.471031</td>\n",
       "      <td>-66.471031</td>\n",
       "      <td>...</td>\n",
       "      <td>-64.386414</td>\n",
       "      <td>-62.351788</td>\n",
       "      <td>-64.222397</td>\n",
       "      <td>-66.152023</td>\n",
       "      <td>-63.642780</td>\n",
       "      <td>-63.769035</td>\n",
       "      <td>-66.471031</td>\n",
       "      <td>-66.471031</td>\n",
       "      <td>-65.012711</td>\n",
       "      <td>female_calm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>-70.152740</td>\n",
       "      <td>-69.913689</td>\n",
       "      <td>-67.810699</td>\n",
       "      <td>-68.664963</td>\n",
       "      <td>-71.470451</td>\n",
       "      <td>-71.470451</td>\n",
       "      <td>-71.470451</td>\n",
       "      <td>-71.470451</td>\n",
       "      <td>-71.470451</td>\n",
       "      <td>-71.470451</td>\n",
       "      <td>...</td>\n",
       "      <td>-43.779472</td>\n",
       "      <td>-43.768883</td>\n",
       "      <td>-44.085487</td>\n",
       "      <td>-43.975651</td>\n",
       "      <td>-44.347027</td>\n",
       "      <td>-44.098366</td>\n",
       "      <td>-46.298855</td>\n",
       "      <td>-47.547016</td>\n",
       "      <td>-46.484295</td>\n",
       "      <td>female_calm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1053</th>\n",
       "      <td>-21.392101</td>\n",
       "      <td>-21.662266</td>\n",
       "      <td>-22.259338</td>\n",
       "      <td>-24.444981</td>\n",
       "      <td>-23.050682</td>\n",
       "      <td>-23.140684</td>\n",
       "      <td>-22.903954</td>\n",
       "      <td>-23.729919</td>\n",
       "      <td>-24.871449</td>\n",
       "      <td>-24.049696</td>\n",
       "      <td>...</td>\n",
       "      <td>-21.701115</td>\n",
       "      <td>-11.972590</td>\n",
       "      <td>-9.322908</td>\n",
       "      <td>-10.145143</td>\n",
       "      <td>-12.772147</td>\n",
       "      <td>-14.352438</td>\n",
       "      <td>-16.328117</td>\n",
       "      <td>-12.141912</td>\n",
       "      <td>-6.584519</td>\n",
       "      <td>male_fearful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>636</th>\n",
       "      <td>-58.458244</td>\n",
       "      <td>-58.458244</td>\n",
       "      <td>-56.495502</td>\n",
       "      <td>-54.816059</td>\n",
       "      <td>-55.097450</td>\n",
       "      <td>-56.864162</td>\n",
       "      <td>-58.341858</td>\n",
       "      <td>-58.458244</td>\n",
       "      <td>-58.458244</td>\n",
       "      <td>-57.983540</td>\n",
       "      <td>...</td>\n",
       "      <td>-56.273376</td>\n",
       "      <td>-55.921200</td>\n",
       "      <td>-55.327827</td>\n",
       "      <td>-56.125870</td>\n",
       "      <td>-54.906818</td>\n",
       "      <td>-55.580368</td>\n",
       "      <td>-55.196865</td>\n",
       "      <td>-55.768448</td>\n",
       "      <td>-57.050266</td>\n",
       "      <td>male_angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>729</th>\n",
       "      <td>-43.144321</td>\n",
       "      <td>-43.240807</td>\n",
       "      <td>-43.574074</td>\n",
       "      <td>-43.559708</td>\n",
       "      <td>-43.377747</td>\n",
       "      <td>-42.510563</td>\n",
       "      <td>-42.398952</td>\n",
       "      <td>-41.682373</td>\n",
       "      <td>-41.117344</td>\n",
       "      <td>-41.384140</td>\n",
       "      <td>...</td>\n",
       "      <td>-32.683876</td>\n",
       "      <td>-33.158852</td>\n",
       "      <td>-33.733822</td>\n",
       "      <td>-35.125347</td>\n",
       "      <td>-37.479668</td>\n",
       "      <td>-36.947750</td>\n",
       "      <td>-36.216888</td>\n",
       "      <td>-34.943890</td>\n",
       "      <td>-37.373890</td>\n",
       "      <td>female_angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1034</th>\n",
       "      <td>-19.608782</td>\n",
       "      <td>-17.110071</td>\n",
       "      <td>-16.288795</td>\n",
       "      <td>-11.379373</td>\n",
       "      <td>-4.384048</td>\n",
       "      <td>-2.022365</td>\n",
       "      <td>-2.588405</td>\n",
       "      <td>-4.300551</td>\n",
       "      <td>-4.970526</td>\n",
       "      <td>-6.814387</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>male_fearful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1057</th>\n",
       "      <td>-25.793974</td>\n",
       "      <td>-25.637810</td>\n",
       "      <td>-25.623514</td>\n",
       "      <td>-24.908133</td>\n",
       "      <td>-24.204029</td>\n",
       "      <td>-23.441559</td>\n",
       "      <td>-22.504555</td>\n",
       "      <td>-23.215464</td>\n",
       "      <td>-23.309681</td>\n",
       "      <td>-22.387897</td>\n",
       "      <td>...</td>\n",
       "      <td>-8.886435</td>\n",
       "      <td>-10.918305</td>\n",
       "      <td>-11.311975</td>\n",
       "      <td>-10.998633</td>\n",
       "      <td>-9.800959</td>\n",
       "      <td>-9.060637</td>\n",
       "      <td>-9.556483</td>\n",
       "      <td>-3.739459</td>\n",
       "      <td>3.027152</td>\n",
       "      <td>male_fearful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344</th>\n",
       "      <td>-46.874268</td>\n",
       "      <td>-47.988571</td>\n",
       "      <td>-48.316895</td>\n",
       "      <td>-48.316895</td>\n",
       "      <td>-48.316895</td>\n",
       "      <td>-48.316895</td>\n",
       "      <td>-48.316895</td>\n",
       "      <td>-48.316895</td>\n",
       "      <td>-48.316895</td>\n",
       "      <td>-48.316895</td>\n",
       "      <td>...</td>\n",
       "      <td>-44.549980</td>\n",
       "      <td>-43.942745</td>\n",
       "      <td>-43.958191</td>\n",
       "      <td>-44.920956</td>\n",
       "      <td>-45.609703</td>\n",
       "      <td>-45.971531</td>\n",
       "      <td>-44.832127</td>\n",
       "      <td>-44.833458</td>\n",
       "      <td>-46.158363</td>\n",
       "      <td>male_happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>-66.002304</td>\n",
       "      <td>-66.021881</td>\n",
       "      <td>-66.983719</td>\n",
       "      <td>-64.977615</td>\n",
       "      <td>-62.615215</td>\n",
       "      <td>-62.309586</td>\n",
       "      <td>-62.451180</td>\n",
       "      <td>-60.948093</td>\n",
       "      <td>-59.981323</td>\n",
       "      <td>-59.848747</td>\n",
       "      <td>...</td>\n",
       "      <td>-46.775917</td>\n",
       "      <td>-48.598087</td>\n",
       "      <td>-48.730759</td>\n",
       "      <td>-47.965942</td>\n",
       "      <td>-49.415318</td>\n",
       "      <td>-50.584885</td>\n",
       "      <td>-53.095871</td>\n",
       "      <td>-55.438869</td>\n",
       "      <td>-54.290329</td>\n",
       "      <td>female_calm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>-62.435238</td>\n",
       "      <td>-60.987801</td>\n",
       "      <td>-60.403915</td>\n",
       "      <td>-60.022331</td>\n",
       "      <td>-59.200264</td>\n",
       "      <td>-59.942162</td>\n",
       "      <td>-59.997593</td>\n",
       "      <td>-58.442886</td>\n",
       "      <td>-58.532730</td>\n",
       "      <td>-60.239304</td>\n",
       "      <td>...</td>\n",
       "      <td>-52.922703</td>\n",
       "      <td>-51.896187</td>\n",
       "      <td>-54.785355</td>\n",
       "      <td>-56.142796</td>\n",
       "      <td>-53.869633</td>\n",
       "      <td>-54.963013</td>\n",
       "      <td>-55.977486</td>\n",
       "      <td>-57.662533</td>\n",
       "      <td>-56.375557</td>\n",
       "      <td>female_calm</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 217 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0          1          2          3          4          5    \\\n",
       "161  -66.471031 -66.464798 -66.471031 -66.471031 -66.471031 -66.471031   \n",
       "101  -70.152740 -69.913689 -67.810699 -68.664963 -71.470451 -71.470451   \n",
       "1053 -21.392101 -21.662266 -22.259338 -24.444981 -23.050682 -23.140684   \n",
       "636  -58.458244 -58.458244 -56.495502 -54.816059 -55.097450 -56.864162   \n",
       "729  -43.144321 -43.240807 -43.574074 -43.559708 -43.377747 -42.510563   \n",
       "1034 -19.608782 -17.110071 -16.288795 -11.379373  -4.384048  -2.022365   \n",
       "1057 -25.793974 -25.637810 -25.623514 -24.908133 -24.204029 -23.441559   \n",
       "344  -46.874268 -47.988571 -48.316895 -48.316895 -48.316895 -48.316895   \n",
       "155  -66.002304 -66.021881 -66.983719 -64.977615 -62.615215 -62.309586   \n",
       "123  -62.435238 -60.987801 -60.403915 -60.022331 -59.200264 -59.942162   \n",
       "\n",
       "            6          7          8          9    ...        207        208  \\\n",
       "161  -66.471031 -66.471031 -66.471031 -66.471031  ... -64.386414 -62.351788   \n",
       "101  -71.470451 -71.470451 -71.470451 -71.470451  ... -43.779472 -43.768883   \n",
       "1053 -22.903954 -23.729919 -24.871449 -24.049696  ... -21.701115 -11.972590   \n",
       "636  -58.341858 -58.458244 -58.458244 -57.983540  ... -56.273376 -55.921200   \n",
       "729  -42.398952 -41.682373 -41.117344 -41.384140  ... -32.683876 -33.158852   \n",
       "1034  -2.588405  -4.300551  -4.970526  -6.814387  ...        NaN        NaN   \n",
       "1057 -22.504555 -23.215464 -23.309681 -22.387897  ...  -8.886435 -10.918305   \n",
       "344  -48.316895 -48.316895 -48.316895 -48.316895  ... -44.549980 -43.942745   \n",
       "155  -62.451180 -60.948093 -59.981323 -59.848747  ... -46.775917 -48.598087   \n",
       "123  -59.997593 -58.442886 -58.532730 -60.239304  ... -52.922703 -51.896187   \n",
       "\n",
       "            209        210        211        212        213        214  \\\n",
       "161  -64.222397 -66.152023 -63.642780 -63.769035 -66.471031 -66.471031   \n",
       "101  -44.085487 -43.975651 -44.347027 -44.098366 -46.298855 -47.547016   \n",
       "1053  -9.322908 -10.145143 -12.772147 -14.352438 -16.328117 -12.141912   \n",
       "636  -55.327827 -56.125870 -54.906818 -55.580368 -55.196865 -55.768448   \n",
       "729  -33.733822 -35.125347 -37.479668 -36.947750 -36.216888 -34.943890   \n",
       "1034        NaN        NaN        NaN        NaN        NaN        NaN   \n",
       "1057 -11.311975 -10.998633  -9.800959  -9.060637  -9.556483  -3.739459   \n",
       "344  -43.958191 -44.920956 -45.609703 -45.971531 -44.832127 -44.833458   \n",
       "155  -48.730759 -47.965942 -49.415318 -50.584885 -53.095871 -55.438869   \n",
       "123  -54.785355 -56.142796 -53.869633 -54.963013 -55.977486 -57.662533   \n",
       "\n",
       "            215           0    \n",
       "161  -65.012711   female_calm  \n",
       "101  -46.484295   female_calm  \n",
       "1053  -6.584519  male_fearful  \n",
       "636  -57.050266    male_angry  \n",
       "729  -37.373890  female_angry  \n",
       "1034        NaN  male_fearful  \n",
       "1057   3.027152  male_fearful  \n",
       "344  -46.158363    male_happy  \n",
       "155  -54.290329   female_calm  \n",
       "123  -56.375557   female_calm  \n",
       "\n",
       "[10 rows x 217 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "rnewdf = shuffle(newdf)\n",
    "rnewdf[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rnewdf=rnewdf.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dividing the data into test and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newdf1 = np.random.rand(len(rnewdf)) < 0.8\n",
    "train = rnewdf[newdf1]\n",
    "test = rnewdf[~newdf1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>207</th>\n",
       "      <th>208</th>\n",
       "      <th>209</th>\n",
       "      <th>210</th>\n",
       "      <th>211</th>\n",
       "      <th>212</th>\n",
       "      <th>213</th>\n",
       "      <th>214</th>\n",
       "      <th>215</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>729</th>\n",
       "      <td>-43.144321</td>\n",
       "      <td>-43.240807</td>\n",
       "      <td>-43.574074</td>\n",
       "      <td>-43.559708</td>\n",
       "      <td>-43.377747</td>\n",
       "      <td>-42.510563</td>\n",
       "      <td>-42.398952</td>\n",
       "      <td>-41.682373</td>\n",
       "      <td>-41.117344</td>\n",
       "      <td>-41.384140</td>\n",
       "      <td>...</td>\n",
       "      <td>-32.683876</td>\n",
       "      <td>-33.158852</td>\n",
       "      <td>-33.733822</td>\n",
       "      <td>-35.125347</td>\n",
       "      <td>-37.479668</td>\n",
       "      <td>-36.947750</td>\n",
       "      <td>-36.216888</td>\n",
       "      <td>-34.943890</td>\n",
       "      <td>-37.373890</td>\n",
       "      <td>female_angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1057</th>\n",
       "      <td>-25.793974</td>\n",
       "      <td>-25.637810</td>\n",
       "      <td>-25.623514</td>\n",
       "      <td>-24.908133</td>\n",
       "      <td>-24.204029</td>\n",
       "      <td>-23.441559</td>\n",
       "      <td>-22.504555</td>\n",
       "      <td>-23.215464</td>\n",
       "      <td>-23.309681</td>\n",
       "      <td>-22.387897</td>\n",
       "      <td>...</td>\n",
       "      <td>-8.886435</td>\n",
       "      <td>-10.918305</td>\n",
       "      <td>-11.311975</td>\n",
       "      <td>-10.998633</td>\n",
       "      <td>-9.800959</td>\n",
       "      <td>-9.060637</td>\n",
       "      <td>-9.556483</td>\n",
       "      <td>-3.739459</td>\n",
       "      <td>3.027152</td>\n",
       "      <td>male_fearful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>898</th>\n",
       "      <td>-42.110771</td>\n",
       "      <td>-44.450138</td>\n",
       "      <td>-43.878517</td>\n",
       "      <td>-43.572796</td>\n",
       "      <td>-44.328049</td>\n",
       "      <td>-47.081234</td>\n",
       "      <td>-44.381809</td>\n",
       "      <td>-43.837570</td>\n",
       "      <td>-44.099277</td>\n",
       "      <td>-43.878906</td>\n",
       "      <td>...</td>\n",
       "      <td>-43.238144</td>\n",
       "      <td>-45.474014</td>\n",
       "      <td>-45.770714</td>\n",
       "      <td>-47.273411</td>\n",
       "      <td>-47.923321</td>\n",
       "      <td>-44.996952</td>\n",
       "      <td>-44.425571</td>\n",
       "      <td>-43.073017</td>\n",
       "      <td>-41.846512</td>\n",
       "      <td>male_fearful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>-53.980492</td>\n",
       "      <td>-52.021362</td>\n",
       "      <td>-52.914917</td>\n",
       "      <td>-53.042610</td>\n",
       "      <td>-52.869320</td>\n",
       "      <td>-55.149212</td>\n",
       "      <td>-60.087891</td>\n",
       "      <td>-56.723209</td>\n",
       "      <td>-54.732597</td>\n",
       "      <td>-56.222458</td>\n",
       "      <td>...</td>\n",
       "      <td>-51.273087</td>\n",
       "      <td>-51.264603</td>\n",
       "      <td>-49.336857</td>\n",
       "      <td>-49.406067</td>\n",
       "      <td>-49.028599</td>\n",
       "      <td>-47.317200</td>\n",
       "      <td>-48.304489</td>\n",
       "      <td>-46.692009</td>\n",
       "      <td>-41.980919</td>\n",
       "      <td>male_happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>983</th>\n",
       "      <td>-27.551817</td>\n",
       "      <td>-25.248302</td>\n",
       "      <td>-22.912870</td>\n",
       "      <td>-23.200481</td>\n",
       "      <td>-24.561083</td>\n",
       "      <td>-24.916075</td>\n",
       "      <td>-24.402773</td>\n",
       "      <td>-23.608076</td>\n",
       "      <td>-24.458662</td>\n",
       "      <td>-26.309509</td>\n",
       "      <td>...</td>\n",
       "      <td>-24.333981</td>\n",
       "      <td>-24.204628</td>\n",
       "      <td>-24.982929</td>\n",
       "      <td>-24.918051</td>\n",
       "      <td>-26.273071</td>\n",
       "      <td>-26.806255</td>\n",
       "      <td>-27.242727</td>\n",
       "      <td>-26.979715</td>\n",
       "      <td>-27.205202</td>\n",
       "      <td>male_angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>-58.327255</td>\n",
       "      <td>-54.953262</td>\n",
       "      <td>-55.248722</td>\n",
       "      <td>-53.967308</td>\n",
       "      <td>-51.944901</td>\n",
       "      <td>-52.322750</td>\n",
       "      <td>-53.839474</td>\n",
       "      <td>-54.893929</td>\n",
       "      <td>-51.611965</td>\n",
       "      <td>-52.376587</td>\n",
       "      <td>...</td>\n",
       "      <td>-43.847286</td>\n",
       "      <td>-43.556648</td>\n",
       "      <td>-42.028385</td>\n",
       "      <td>-40.557636</td>\n",
       "      <td>-43.097683</td>\n",
       "      <td>-42.761173</td>\n",
       "      <td>-42.750996</td>\n",
       "      <td>-45.633850</td>\n",
       "      <td>-44.306820</td>\n",
       "      <td>male_calm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>856</th>\n",
       "      <td>-45.593971</td>\n",
       "      <td>-45.288883</td>\n",
       "      <td>-43.909744</td>\n",
       "      <td>-43.639015</td>\n",
       "      <td>-44.616287</td>\n",
       "      <td>-43.998116</td>\n",
       "      <td>-43.588974</td>\n",
       "      <td>-44.078384</td>\n",
       "      <td>-43.900879</td>\n",
       "      <td>-42.735794</td>\n",
       "      <td>...</td>\n",
       "      <td>-40.460434</td>\n",
       "      <td>-40.520290</td>\n",
       "      <td>-40.441990</td>\n",
       "      <td>-41.478268</td>\n",
       "      <td>-42.777439</td>\n",
       "      <td>-41.823460</td>\n",
       "      <td>-41.062496</td>\n",
       "      <td>-40.644089</td>\n",
       "      <td>-42.000237</td>\n",
       "      <td>male_fearful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487</th>\n",
       "      <td>-52.527290</td>\n",
       "      <td>-52.740135</td>\n",
       "      <td>-51.810162</td>\n",
       "      <td>-50.845955</td>\n",
       "      <td>-52.180595</td>\n",
       "      <td>-50.952663</td>\n",
       "      <td>-51.928787</td>\n",
       "      <td>-50.068733</td>\n",
       "      <td>-51.063869</td>\n",
       "      <td>-51.178474</td>\n",
       "      <td>...</td>\n",
       "      <td>-52.454109</td>\n",
       "      <td>-53.226479</td>\n",
       "      <td>-51.588139</td>\n",
       "      <td>-48.858006</td>\n",
       "      <td>-49.559284</td>\n",
       "      <td>-49.241310</td>\n",
       "      <td>-49.745724</td>\n",
       "      <td>-50.692467</td>\n",
       "      <td>-49.275898</td>\n",
       "      <td>female_sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>-53.141991</td>\n",
       "      <td>-52.674992</td>\n",
       "      <td>-50.214226</td>\n",
       "      <td>-50.819004</td>\n",
       "      <td>-54.603825</td>\n",
       "      <td>-52.255264</td>\n",
       "      <td>-50.683800</td>\n",
       "      <td>-52.724796</td>\n",
       "      <td>-49.598228</td>\n",
       "      <td>-51.657570</td>\n",
       "      <td>...</td>\n",
       "      <td>-47.976353</td>\n",
       "      <td>-47.613281</td>\n",
       "      <td>-48.414055</td>\n",
       "      <td>-49.194748</td>\n",
       "      <td>-50.277706</td>\n",
       "      <td>-52.465252</td>\n",
       "      <td>-54.078613</td>\n",
       "      <td>-56.517986</td>\n",
       "      <td>-57.023308</td>\n",
       "      <td>male_happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>883</th>\n",
       "      <td>-39.097198</td>\n",
       "      <td>-38.488297</td>\n",
       "      <td>-37.440987</td>\n",
       "      <td>-37.764202</td>\n",
       "      <td>-36.837551</td>\n",
       "      <td>-36.836800</td>\n",
       "      <td>-36.750015</td>\n",
       "      <td>-37.506802</td>\n",
       "      <td>-38.380180</td>\n",
       "      <td>-38.947472</td>\n",
       "      <td>...</td>\n",
       "      <td>-29.992664</td>\n",
       "      <td>-29.034035</td>\n",
       "      <td>-28.192539</td>\n",
       "      <td>-30.632677</td>\n",
       "      <td>-31.732262</td>\n",
       "      <td>-32.373383</td>\n",
       "      <td>-31.606991</td>\n",
       "      <td>-31.457842</td>\n",
       "      <td>-27.050022</td>\n",
       "      <td>female_fearful</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>256 rows × 217 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0          1          2          3          4          5    \\\n",
       "729  -43.144321 -43.240807 -43.574074 -43.559708 -43.377747 -42.510563   \n",
       "1057 -25.793974 -25.637810 -25.623514 -24.908133 -24.204029 -23.441559   \n",
       "898  -42.110771 -44.450138 -43.878517 -43.572796 -44.328049 -47.081234   \n",
       "230  -53.980492 -52.021362 -52.914917 -53.042610 -52.869320 -55.149212   \n",
       "983  -27.551817 -25.248302 -22.912870 -23.200481 -24.561083 -24.916075   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "112  -58.327255 -54.953262 -55.248722 -53.967308 -51.944901 -52.322750   \n",
       "856  -45.593971 -45.288883 -43.909744 -43.639015 -44.616287 -43.998116   \n",
       "487  -52.527290 -52.740135 -51.810162 -50.845955 -52.180595 -50.952663   \n",
       "256  -53.141991 -52.674992 -50.214226 -50.819004 -54.603825 -52.255264   \n",
       "883  -39.097198 -38.488297 -37.440987 -37.764202 -36.837551 -36.836800   \n",
       "\n",
       "            6          7          8          9    ...        207        208  \\\n",
       "729  -42.398952 -41.682373 -41.117344 -41.384140  ... -32.683876 -33.158852   \n",
       "1057 -22.504555 -23.215464 -23.309681 -22.387897  ...  -8.886435 -10.918305   \n",
       "898  -44.381809 -43.837570 -44.099277 -43.878906  ... -43.238144 -45.474014   \n",
       "230  -60.087891 -56.723209 -54.732597 -56.222458  ... -51.273087 -51.264603   \n",
       "983  -24.402773 -23.608076 -24.458662 -26.309509  ... -24.333981 -24.204628   \n",
       "...         ...        ...        ...        ...  ...        ...        ...   \n",
       "112  -53.839474 -54.893929 -51.611965 -52.376587  ... -43.847286 -43.556648   \n",
       "856  -43.588974 -44.078384 -43.900879 -42.735794  ... -40.460434 -40.520290   \n",
       "487  -51.928787 -50.068733 -51.063869 -51.178474  ... -52.454109 -53.226479   \n",
       "256  -50.683800 -52.724796 -49.598228 -51.657570  ... -47.976353 -47.613281   \n",
       "883  -36.750015 -37.506802 -38.380180 -38.947472  ... -29.992664 -29.034035   \n",
       "\n",
       "            209        210        211        212        213        214  \\\n",
       "729  -33.733822 -35.125347 -37.479668 -36.947750 -36.216888 -34.943890   \n",
       "1057 -11.311975 -10.998633  -9.800959  -9.060637  -9.556483  -3.739459   \n",
       "898  -45.770714 -47.273411 -47.923321 -44.996952 -44.425571 -43.073017   \n",
       "230  -49.336857 -49.406067 -49.028599 -47.317200 -48.304489 -46.692009   \n",
       "983  -24.982929 -24.918051 -26.273071 -26.806255 -27.242727 -26.979715   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "112  -42.028385 -40.557636 -43.097683 -42.761173 -42.750996 -45.633850   \n",
       "856  -40.441990 -41.478268 -42.777439 -41.823460 -41.062496 -40.644089   \n",
       "487  -51.588139 -48.858006 -49.559284 -49.241310 -49.745724 -50.692467   \n",
       "256  -48.414055 -49.194748 -50.277706 -52.465252 -54.078613 -56.517986   \n",
       "883  -28.192539 -30.632677 -31.732262 -32.373383 -31.606991 -31.457842   \n",
       "\n",
       "            215             0    \n",
       "729  -37.373890    female_angry  \n",
       "1057   3.027152    male_fearful  \n",
       "898  -41.846512    male_fearful  \n",
       "230  -41.980919      male_happy  \n",
       "983  -27.205202      male_angry  \n",
       "...         ...             ...  \n",
       "112  -44.306820       male_calm  \n",
       "856  -42.000237    male_fearful  \n",
       "487  -49.275898      female_sad  \n",
       "256  -57.023308      male_happy  \n",
       "883  -27.050022  female_fearful  \n",
       "\n",
       "[256 rows x 217 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainfeatures = train.iloc[:, :-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainlabel = train.iloc[:, -1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testfeatures = test.iloc[:, :-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testlabel = test.iloc[:, -1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import np_utils\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "X_train = np.array(trainfeatures)\n",
    "y_train = np.array(trainlabel)\n",
    "X_test = np.array(testfeatures)\n",
    "y_test = np.array(testlabel)\n",
    "\n",
    "lb = LabelEncoder()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(944, 216)\n",
      "(944, 1)\n",
      "(256, 216)\n",
      "(256, 1)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Simas\\Miniconda3\\envs\\med4\\lib\\site-packages\\sklearn\\utils\\validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "y_train = np_utils.to_categorical(lb.fit_transform(y_train))\n",
    "y_test = np_utils.to_categorical(lb.fit_transform(y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(944, 216)\n",
      "(944, 10)\n",
      "(256, 216)\n",
      "(256, 10)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(944, 216)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding sequence for CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pad sequences\n"
     ]
    }
   ],
   "source": [
    "print('Pad sequences')\n",
    "x_traincnn = np.expand_dims(X_train, axis=2)\n",
    "x_testcnn= np.expand_dims(X_test, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv1D(128, 5,padding='same',\n",
    "                 input_shape=(216,1)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv1D(128, 5,padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(MaxPooling1D(pool_size=(8)))\n",
    "model.add(Conv1D(128, 5,padding='same',))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv1D(128, 5,padding='same',))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv1D(128, 5,padding='same',))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv1D(128, 5,padding='same',))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))\n",
    "opt = tf.keras.optimizers.RMSprop(lr=0.00001, decay=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 216, 128)          768       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 216, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 216, 128)          82048     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 216, 128)          0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 216, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 27, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 27, 128)           82048     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 27, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 27, 128)           82048     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 27, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 27, 128)           82048     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 27, 128)           0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 27, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 27, 128)           82048     \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 27, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 3456)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 10)                34570     \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 445,578\n",
      "Trainable params: 445,578\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',optimizer=opt,metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1009\n",
      "30/30 [==============================] - 3s 72ms/step - loss: 2.5833 - accuracy: 0.0660 - val_loss: 2.3190 - val_accuracy: 0.0352\n",
      "Epoch 2/1009\n",
      "30/30 [==============================] - 2s 59ms/step - loss: 2.3503 - accuracy: 0.0930 - val_loss: 2.2990 - val_accuracy: 0.0977\n",
      "Epoch 3/1009\n",
      "30/30 [==============================] - 2s 59ms/step - loss: 2.3261 - accuracy: 0.1183 - val_loss: 2.2870 - val_accuracy: 0.1562\n",
      "Epoch 4/1009\n",
      "30/30 [==============================] - 2s 59ms/step - loss: 2.3096 - accuracy: 0.1308 - val_loss: 2.2814 - val_accuracy: 0.1719\n",
      "Epoch 5/1009\n",
      "30/30 [==============================] - 2s 59ms/step - loss: 2.2674 - accuracy: 0.1674 - val_loss: 2.2692 - val_accuracy: 0.1758\n",
      "Epoch 6/1009\n",
      "30/30 [==============================] - 2s 62ms/step - loss: 2.2860 - accuracy: 0.1360 - val_loss: 2.2620 - val_accuracy: 0.1953\n",
      "Epoch 7/1009\n",
      "30/30 [==============================] - 2s 59ms/step - loss: 2.2682 - accuracy: 0.1410 - val_loss: 2.2581 - val_accuracy: 0.1875\n",
      "Epoch 8/1009\n",
      "30/30 [==============================] - 2s 59ms/step - loss: 2.2645 - accuracy: 0.1311 - val_loss: 2.2481 - val_accuracy: 0.1953\n",
      "Epoch 9/1009\n",
      "30/30 [==============================] - 2s 61ms/step - loss: 2.2511 - accuracy: 0.1574 - val_loss: 2.2385 - val_accuracy: 0.2305\n",
      "Epoch 10/1009\n",
      "30/30 [==============================] - 2s 67ms/step - loss: 2.2412 - accuracy: 0.1751 - val_loss: 2.2289 - val_accuracy: 0.2070\n",
      "Epoch 11/1009\n",
      "30/30 [==============================] - 2s 67ms/step - loss: 2.2331 - accuracy: 0.1534 - val_loss: 2.2259 - val_accuracy: 0.2188\n",
      "Epoch 12/1009\n",
      "30/30 [==============================] - 2s 66ms/step - loss: 2.2310 - accuracy: 0.1726 - val_loss: 2.2121 - val_accuracy: 0.2266\n",
      "Epoch 13/1009\n",
      "30/30 [==============================] - 2s 67ms/step - loss: 2.2048 - accuracy: 0.1824 - val_loss: 2.1993 - val_accuracy: 0.2344\n",
      "Epoch 14/1009\n",
      "30/30 [==============================] - 2s 66ms/step - loss: 2.2045 - accuracy: 0.1777 - val_loss: 2.1925 - val_accuracy: 0.2344\n",
      "Epoch 15/1009\n",
      "30/30 [==============================] - 2s 66ms/step - loss: 2.1871 - accuracy: 0.1934 - val_loss: 2.1822 - val_accuracy: 0.2461\n",
      "Epoch 16/1009\n",
      "30/30 [==============================] - 2s 66ms/step - loss: 2.1652 - accuracy: 0.1987 - val_loss: 2.1707 - val_accuracy: 0.2695\n",
      "Epoch 17/1009\n",
      "30/30 [==============================] - 2s 68ms/step - loss: 2.1732 - accuracy: 0.1790 - val_loss: 2.1605 - val_accuracy: 0.2500\n",
      "Epoch 18/1009\n",
      "30/30 [==============================] - 2s 66ms/step - loss: 2.1440 - accuracy: 0.2593 - val_loss: 2.1472 - val_accuracy: 0.2578\n",
      "Epoch 19/1009\n",
      "30/30 [==============================] - 2s 66ms/step - loss: 2.1529 - accuracy: 0.2143 - val_loss: 2.1291 - val_accuracy: 0.2812\n",
      "Epoch 20/1009\n",
      "30/30 [==============================] - 2s 66ms/step - loss: 2.1216 - accuracy: 0.2246 - val_loss: 2.1179 - val_accuracy: 0.2500\n",
      "Epoch 21/1009\n",
      "30/30 [==============================] - 2s 66ms/step - loss: 2.1122 - accuracy: 0.2210 - val_loss: 2.1047 - val_accuracy: 0.2617\n",
      "Epoch 22/1009\n",
      "30/30 [==============================] - 2s 66ms/step - loss: 2.1048 - accuracy: 0.2200 - val_loss: 2.0832 - val_accuracy: 0.2500\n",
      "Epoch 23/1009\n",
      "30/30 [==============================] - 2s 72ms/step - loss: 2.0830 - accuracy: 0.2226 - val_loss: 2.0764 - val_accuracy: 0.2344\n",
      "Epoch 24/1009\n",
      "30/30 [==============================] - 2s 83ms/step - loss: 2.0301 - accuracy: 0.2531 - val_loss: 2.0509 - val_accuracy: 0.2891\n",
      "Epoch 25/1009\n",
      "30/30 [==============================] - 2s 80ms/step - loss: 2.0278 - accuracy: 0.2417 - val_loss: 2.0393 - val_accuracy: 0.2695\n",
      "Epoch 26/1009\n",
      "30/30 [==============================] - 3s 89ms/step - loss: 2.0390 - accuracy: 0.2480 - val_loss: 2.0223 - val_accuracy: 0.2734\n",
      "Epoch 27/1009\n",
      "30/30 [==============================] - 2s 74ms/step - loss: 1.9876 - accuracy: 0.2836 - val_loss: 1.9893 - val_accuracy: 0.3164\n",
      "Epoch 28/1009\n",
      "30/30 [==============================] - 2s 75ms/step - loss: 1.9793 - accuracy: 0.2496 - val_loss: 1.9714 - val_accuracy: 0.3086\n",
      "Epoch 29/1009\n",
      "30/30 [==============================] - 3s 90ms/step - loss: 1.9429 - accuracy: 0.3103 - val_loss: 1.9551 - val_accuracy: 0.3164\n",
      "Epoch 30/1009\n",
      "30/30 [==============================] - 2s 72ms/step - loss: 1.9318 - accuracy: 0.2704 - val_loss: 1.9529 - val_accuracy: 0.2930\n",
      "Epoch 31/1009\n",
      "30/30 [==============================] - 2s 73ms/step - loss: 1.9272 - accuracy: 0.2843 - val_loss: 1.9211 - val_accuracy: 0.3398\n",
      "Epoch 32/1009\n",
      "30/30 [==============================] - 2s 71ms/step - loss: 1.8882 - accuracy: 0.3000 - val_loss: 1.8995 - val_accuracy: 0.3438\n",
      "Epoch 33/1009\n",
      "30/30 [==============================] - 2s 72ms/step - loss: 1.8449 - accuracy: 0.3209 - val_loss: 1.8821 - val_accuracy: 0.3359\n",
      "Epoch 34/1009\n",
      "30/30 [==============================] - 2s 72ms/step - loss: 1.8612 - accuracy: 0.3007 - val_loss: 1.8752 - val_accuracy: 0.3125\n",
      "Epoch 35/1009\n",
      "30/30 [==============================] - 2s 71ms/step - loss: 1.8257 - accuracy: 0.3420 - val_loss: 1.8536 - val_accuracy: 0.3164\n",
      "Epoch 36/1009\n",
      "30/30 [==============================] - 2s 74ms/step - loss: 1.8406 - accuracy: 0.2982 - val_loss: 1.8375 - val_accuracy: 0.3438\n",
      "Epoch 37/1009\n",
      "30/30 [==============================] - 2s 72ms/step - loss: 1.8219 - accuracy: 0.3204 - val_loss: 1.8197 - val_accuracy: 0.3281\n",
      "Epoch 38/1009\n",
      "30/30 [==============================] - 2s 72ms/step - loss: 1.8011 - accuracy: 0.3389 - val_loss: 1.8242 - val_accuracy: 0.3320\n",
      "Epoch 39/1009\n",
      "30/30 [==============================] - 2s 74ms/step - loss: 1.7929 - accuracy: 0.2876 - val_loss: 1.7941 - val_accuracy: 0.3516\n",
      "Epoch 40/1009\n",
      "30/30 [==============================] - 2s 72ms/step - loss: 1.7527 - accuracy: 0.3031 - val_loss: 1.7822 - val_accuracy: 0.3398\n",
      "Epoch 41/1009\n",
      "30/30 [==============================] - 2s 72ms/step - loss: 1.7701 - accuracy: 0.2974 - val_loss: 1.7914 - val_accuracy: 0.3477\n",
      "Epoch 42/1009\n",
      "30/30 [==============================] - 2s 74ms/step - loss: 1.7479 - accuracy: 0.2945 - val_loss: 1.7753 - val_accuracy: 0.3281\n",
      "Epoch 43/1009\n",
      "30/30 [==============================] - 2s 71ms/step - loss: 1.7217 - accuracy: 0.3066 - val_loss: 1.7811 - val_accuracy: 0.3125\n",
      "Epoch 44/1009\n",
      "30/30 [==============================] - 2s 72ms/step - loss: 1.7274 - accuracy: 0.3376 - val_loss: 1.7372 - val_accuracy: 0.3281\n",
      "Epoch 45/1009\n",
      "30/30 [==============================] - 2s 71ms/step - loss: 1.6887 - accuracy: 0.3591 - val_loss: 1.7878 - val_accuracy: 0.3203\n",
      "Epoch 46/1009\n",
      "30/30 [==============================] - 2s 72ms/step - loss: 1.6869 - accuracy: 0.3315 - val_loss: 1.7413 - val_accuracy: 0.3516\n",
      "Epoch 47/1009\n",
      "30/30 [==============================] - 2s 74ms/step - loss: 1.6775 - accuracy: 0.3434 - val_loss: 1.7206 - val_accuracy: 0.3555\n",
      "Epoch 48/1009\n",
      "30/30 [==============================] - 2s 72ms/step - loss: 1.6701 - accuracy: 0.3399 - val_loss: 1.7559 - val_accuracy: 0.3438\n",
      "Epoch 49/1009\n",
      "30/30 [==============================] - 2s 72ms/step - loss: 1.6640 - accuracy: 0.3758 - val_loss: 1.7094 - val_accuracy: 0.3555\n",
      "Epoch 50/1009\n",
      "30/30 [==============================] - 2s 73ms/step - loss: 1.6548 - accuracy: 0.3767 - val_loss: 1.7423 - val_accuracy: 0.2930\n",
      "Epoch 51/1009\n",
      "30/30 [==============================] - 2s 72ms/step - loss: 1.6354 - accuracy: 0.3555 - val_loss: 1.7006 - val_accuracy: 0.3555\n",
      "Epoch 52/1009\n",
      "30/30 [==============================] - 2s 72ms/step - loss: 1.6419 - accuracy: 0.3734 - val_loss: 1.6781 - val_accuracy: 0.3555\n",
      "Epoch 53/1009\n",
      "30/30 [==============================] - 2s 71ms/step - loss: 1.6470 - accuracy: 0.3599 - val_loss: 1.7115 - val_accuracy: 0.3438\n",
      "Epoch 54/1009\n",
      "30/30 [==============================] - 2s 73ms/step - loss: 1.6464 - accuracy: 0.3522 - val_loss: 1.6988 - val_accuracy: 0.3555\n",
      "Epoch 55/1009\n",
      "30/30 [==============================] - 2s 71ms/step - loss: 1.6213 - accuracy: 0.3689 - val_loss: 1.6804 - val_accuracy: 0.3594\n",
      "Epoch 56/1009\n",
      "30/30 [==============================] - 2s 71ms/step - loss: 1.6243 - accuracy: 0.3483 - val_loss: 1.6939 - val_accuracy: 0.3633\n",
      "Epoch 57/1009\n",
      "30/30 [==============================] - 2s 73ms/step - loss: 1.6632 - accuracy: 0.3751 - val_loss: 1.6608 - val_accuracy: 0.3711\n",
      "Epoch 58/1009\n",
      "30/30 [==============================] - 2s 75ms/step - loss: 1.6061 - accuracy: 0.3704 - val_loss: 1.6544 - val_accuracy: 0.3906\n",
      "Epoch 59/1009\n",
      "30/30 [==============================] - 2s 74ms/step - loss: 1.5913 - accuracy: 0.3785 - val_loss: 1.6414 - val_accuracy: 0.3867\n",
      "Epoch 60/1009\n",
      "30/30 [==============================] - 2s 71ms/step - loss: 1.5867 - accuracy: 0.3917 - val_loss: 1.6384 - val_accuracy: 0.3984\n",
      "Epoch 61/1009\n",
      "30/30 [==============================] - 2s 73ms/step - loss: 1.5765 - accuracy: 0.4165 - val_loss: 1.6411 - val_accuracy: 0.3633\n",
      "Epoch 62/1009\n",
      "30/30 [==============================] - 2s 71ms/step - loss: 1.5753 - accuracy: 0.3757 - val_loss: 1.6362 - val_accuracy: 0.3867\n",
      "Epoch 63/1009\n",
      "30/30 [==============================] - 2s 72ms/step - loss: 1.6110 - accuracy: 0.3786 - val_loss: 1.6368 - val_accuracy: 0.3789\n",
      "Epoch 64/1009\n",
      "30/30 [==============================] - 2s 71ms/step - loss: 1.5819 - accuracy: 0.3791 - val_loss: 1.6497 - val_accuracy: 0.3477\n",
      "Epoch 65/1009\n",
      "30/30 [==============================] - 2s 76ms/step - loss: 1.5943 - accuracy: 0.3736 - val_loss: 1.6150 - val_accuracy: 0.4062\n",
      "Epoch 66/1009\n",
      "30/30 [==============================] - 2s 71ms/step - loss: 1.5769 - accuracy: 0.3735 - val_loss: 1.6139 - val_accuracy: 0.3945\n",
      "Epoch 67/1009\n",
      "30/30 [==============================] - 2s 75ms/step - loss: 1.5345 - accuracy: 0.3976 - val_loss: 1.6089 - val_accuracy: 0.4141\n",
      "Epoch 68/1009\n",
      "30/30 [==============================] - 2s 73ms/step - loss: 1.5763 - accuracy: 0.3813 - val_loss: 1.6074 - val_accuracy: 0.3945\n",
      "Epoch 69/1009\n",
      "30/30 [==============================] - 2s 77ms/step - loss: 1.5611 - accuracy: 0.4135 - val_loss: 1.6141 - val_accuracy: 0.3789\n",
      "Epoch 70/1009\n",
      "30/30 [==============================] - 2s 75ms/step - loss: 1.5394 - accuracy: 0.4106 - val_loss: 1.6450 - val_accuracy: 0.3477\n",
      "Epoch 71/1009\n",
      "30/30 [==============================] - 2s 73ms/step - loss: 1.5337 - accuracy: 0.4042 - val_loss: 1.5902 - val_accuracy: 0.4062\n",
      "Epoch 72/1009\n",
      "30/30 [==============================] - 2s 73ms/step - loss: 1.5010 - accuracy: 0.4328 - val_loss: 1.6049 - val_accuracy: 0.4180\n",
      "Epoch 73/1009\n",
      "30/30 [==============================] - 2s 74ms/step - loss: 1.5082 - accuracy: 0.4346 - val_loss: 1.6058 - val_accuracy: 0.3867\n",
      "Epoch 74/1009\n",
      "30/30 [==============================] - 2s 72ms/step - loss: 1.5550 - accuracy: 0.3844 - val_loss: 1.5938 - val_accuracy: 0.4180\n",
      "Epoch 75/1009\n",
      "30/30 [==============================] - 2s 78ms/step - loss: 1.5426 - accuracy: 0.4124 - val_loss: 1.5866 - val_accuracy: 0.3906\n",
      "Epoch 76/1009\n",
      "30/30 [==============================] - 2s 76ms/step - loss: 1.5188 - accuracy: 0.4263 - val_loss: 1.5810 - val_accuracy: 0.4102\n",
      "Epoch 77/1009\n",
      "30/30 [==============================] - 2s 71ms/step - loss: 1.5196 - accuracy: 0.4167 - val_loss: 1.5904 - val_accuracy: 0.4102\n",
      "Epoch 78/1009\n",
      "30/30 [==============================] - 2s 72ms/step - loss: 1.4941 - accuracy: 0.4322 - val_loss: 1.5757 - val_accuracy: 0.3984\n",
      "Epoch 79/1009\n",
      "30/30 [==============================] - 2s 71ms/step - loss: 1.5412 - accuracy: 0.3964 - val_loss: 1.6186 - val_accuracy: 0.4102\n",
      "Epoch 80/1009\n",
      "30/30 [==============================] - 2s 72ms/step - loss: 1.5317 - accuracy: 0.4098 - val_loss: 1.5664 - val_accuracy: 0.4180\n",
      "Epoch 81/1009\n",
      "30/30 [==============================] - 2s 72ms/step - loss: 1.5013 - accuracy: 0.4418 - val_loss: 1.6008 - val_accuracy: 0.3984\n",
      "Epoch 82/1009\n",
      "30/30 [==============================] - 2s 71ms/step - loss: 1.4791 - accuracy: 0.4179 - val_loss: 1.5768 - val_accuracy: 0.4141\n",
      "Epoch 83/1009\n",
      "30/30 [==============================] - 2s 71ms/step - loss: 1.4606 - accuracy: 0.4388 - val_loss: 1.5596 - val_accuracy: 0.4258\n",
      "Epoch 84/1009\n",
      "30/30 [==============================] - 2s 71ms/step - loss: 1.5085 - accuracy: 0.4221 - val_loss: 1.5639 - val_accuracy: 0.3906\n",
      "Epoch 85/1009\n",
      "30/30 [==============================] - 2s 72ms/step - loss: 1.4526 - accuracy: 0.4131 - val_loss: 1.5940 - val_accuracy: 0.4062\n",
      "Epoch 86/1009\n",
      "30/30 [==============================] - 2s 74ms/step - loss: 1.4807 - accuracy: 0.4351 - val_loss: 1.5937 - val_accuracy: 0.3828\n",
      "Epoch 87/1009\n",
      "30/30 [==============================] - 2s 73ms/step - loss: 1.4897 - accuracy: 0.4242 - val_loss: 1.6166 - val_accuracy: 0.4180\n",
      "Epoch 88/1009\n",
      "30/30 [==============================] - 2s 71ms/step - loss: 1.4436 - accuracy: 0.4342 - val_loss: 1.5510 - val_accuracy: 0.4219\n",
      "Epoch 89/1009\n",
      "30/30 [==============================] - 2s 72ms/step - loss: 1.4460 - accuracy: 0.4503 - val_loss: 1.5398 - val_accuracy: 0.4141\n",
      "Epoch 90/1009\n",
      "30/30 [==============================] - 2s 71ms/step - loss: 1.4602 - accuracy: 0.4392 - val_loss: 1.5683 - val_accuracy: 0.4102\n",
      "Epoch 91/1009\n",
      "30/30 [==============================] - 2s 72ms/step - loss: 1.5021 - accuracy: 0.4126 - val_loss: 1.5505 - val_accuracy: 0.4297\n",
      "Epoch 92/1009\n",
      "30/30 [==============================] - 2s 71ms/step - loss: 1.4198 - accuracy: 0.4580 - val_loss: 1.5380 - val_accuracy: 0.4297\n",
      "Epoch 93/1009\n",
      "30/30 [==============================] - 2s 73ms/step - loss: 1.4591 - accuracy: 0.4510 - val_loss: 1.5503 - val_accuracy: 0.4180\n",
      "Epoch 94/1009\n",
      "30/30 [==============================] - 2s 71ms/step - loss: 1.4060 - accuracy: 0.4755 - val_loss: 1.5509 - val_accuracy: 0.4219\n",
      "Epoch 95/1009\n",
      "30/30 [==============================] - 2s 71ms/step - loss: 1.4614 - accuracy: 0.4584 - val_loss: 1.5374 - val_accuracy: 0.4141\n",
      "Epoch 96/1009\n",
      "30/30 [==============================] - 2s 72ms/step - loss: 1.4105 - accuracy: 0.4564 - val_loss: 1.5406 - val_accuracy: 0.4258\n",
      "Epoch 97/1009\n",
      "30/30 [==============================] - 2s 73ms/step - loss: 1.4155 - accuracy: 0.4628 - val_loss: 1.5472 - val_accuracy: 0.4141\n",
      "Epoch 98/1009\n",
      "30/30 [==============================] - 2s 72ms/step - loss: 1.4567 - accuracy: 0.4425 - val_loss: 1.5322 - val_accuracy: 0.4297\n",
      "Epoch 99/1009\n",
      "30/30 [==============================] - 2s 72ms/step - loss: 1.4269 - accuracy: 0.4468 - val_loss: 1.5517 - val_accuracy: 0.4062\n",
      "Epoch 100/1009\n",
      "30/30 [==============================] - 2s 72ms/step - loss: 1.4482 - accuracy: 0.4312 - val_loss: 1.5324 - val_accuracy: 0.4102\n",
      "Epoch 101/1009\n",
      "30/30 [==============================] - 2s 73ms/step - loss: 1.3988 - accuracy: 0.4553 - val_loss: 1.5223 - val_accuracy: 0.4336\n",
      "Epoch 102/1009\n",
      "30/30 [==============================] - 2s 72ms/step - loss: 1.3641 - accuracy: 0.4761 - val_loss: 1.5435 - val_accuracy: 0.4219\n",
      "Epoch 103/1009\n",
      "30/30 [==============================] - 2s 71ms/step - loss: 1.3690 - accuracy: 0.5007 - val_loss: 1.5247 - val_accuracy: 0.4102\n",
      "Epoch 104/1009\n",
      "30/30 [==============================] - 2s 71ms/step - loss: 1.3793 - accuracy: 0.4606 - val_loss: 1.5194 - val_accuracy: 0.4219\n",
      "Epoch 105/1009\n",
      "30/30 [==============================] - 2s 71ms/step - loss: 1.3479 - accuracy: 0.4900 - val_loss: 1.5188 - val_accuracy: 0.4336\n",
      "Epoch 106/1009\n",
      "30/30 [==============================] - 2s 73ms/step - loss: 1.3938 - accuracy: 0.4653 - val_loss: 1.5219 - val_accuracy: 0.4297\n",
      "Epoch 107/1009\n",
      "30/30 [==============================] - 2s 71ms/step - loss: 1.3862 - accuracy: 0.4737 - val_loss: 1.5391 - val_accuracy: 0.4141\n",
      "Epoch 108/1009\n",
      "30/30 [==============================] - 2s 71ms/step - loss: 1.3745 - accuracy: 0.4806 - val_loss: 1.5458 - val_accuracy: 0.4141\n",
      "Epoch 109/1009\n",
      "30/30 [==============================] - 2s 71ms/step - loss: 1.3933 - accuracy: 0.4682 - val_loss: 1.5821 - val_accuracy: 0.3828\n",
      "Epoch 110/1009\n",
      "30/30 [==============================] - 2s 73ms/step - loss: 1.4025 - accuracy: 0.4574 - val_loss: 1.5882 - val_accuracy: 0.3828\n",
      "Epoch 111/1009\n",
      "30/30 [==============================] - 2s 71ms/step - loss: 1.4061 - accuracy: 0.4545 - val_loss: 1.5639 - val_accuracy: 0.4102\n",
      "Epoch 112/1009\n",
      "30/30 [==============================] - 2s 71ms/step - loss: 1.3944 - accuracy: 0.4809 - val_loss: 1.5687 - val_accuracy: 0.4102\n",
      "Epoch 113/1009\n",
      "30/30 [==============================] - 2s 73ms/step - loss: 1.3466 - accuracy: 0.4973 - val_loss: 1.5148 - val_accuracy: 0.4297\n",
      "Epoch 114/1009\n",
      "30/30 [==============================] - 2s 71ms/step - loss: 1.3388 - accuracy: 0.4964 - val_loss: 1.5153 - val_accuracy: 0.4297\n",
      "Epoch 115/1009\n",
      "30/30 [==============================] - 2s 71ms/step - loss: 1.3671 - accuracy: 0.4807 - val_loss: 1.5242 - val_accuracy: 0.4023\n",
      "Epoch 116/1009\n",
      "30/30 [==============================] - 2s 71ms/step - loss: 1.3842 - accuracy: 0.4698 - val_loss: 1.5324 - val_accuracy: 0.4062\n",
      "Epoch 117/1009\n",
      "30/30 [==============================] - 2s 79ms/step - loss: 1.3591 - accuracy: 0.4697 - val_loss: 1.5763 - val_accuracy: 0.4102\n",
      "Epoch 118/1009\n",
      "30/30 [==============================] - 2s 76ms/step - loss: 1.4044 - accuracy: 0.4842 - val_loss: 1.5168 - val_accuracy: 0.4180\n",
      "Epoch 119/1009\n",
      "30/30 [==============================] - 2s 73ms/step - loss: 1.3912 - accuracy: 0.4763 - val_loss: 1.5110 - val_accuracy: 0.4219\n",
      "Epoch 120/1009\n",
      "30/30 [==============================] - 2s 73ms/step - loss: 1.3600 - accuracy: 0.4825 - val_loss: 1.4874 - val_accuracy: 0.4258\n",
      "Epoch 121/1009\n",
      "30/30 [==============================] - 2s 72ms/step - loss: 1.3389 - accuracy: 0.4978 - val_loss: 1.5005 - val_accuracy: 0.4219\n",
      "Epoch 122/1009\n",
      "30/30 [==============================] - 2s 70ms/step - loss: 1.3567 - accuracy: 0.4805 - val_loss: 1.4953 - val_accuracy: 0.4336\n",
      "Epoch 123/1009\n",
      "30/30 [==============================] - 2s 79ms/step - loss: 1.3275 - accuracy: 0.4955 - val_loss: 1.5298 - val_accuracy: 0.4180\n",
      "Epoch 124/1009\n",
      "30/30 [==============================] - 2s 79ms/step - loss: 1.3233 - accuracy: 0.5011 - val_loss: 1.4913 - val_accuracy: 0.4375\n",
      "Epoch 125/1009\n",
      "30/30 [==============================] - 2s 70ms/step - loss: 1.3172 - accuracy: 0.5274 - val_loss: 1.5448 - val_accuracy: 0.4062\n",
      "Epoch 126/1009\n",
      "30/30 [==============================] - 2s 80ms/step - loss: 1.3839 - accuracy: 0.4827 - val_loss: 1.5013 - val_accuracy: 0.4219\n",
      "Epoch 127/1009\n",
      "30/30 [==============================] - 2s 69ms/step - loss: 1.3258 - accuracy: 0.4811 - val_loss: 1.5023 - val_accuracy: 0.4219\n",
      "Epoch 128/1009\n",
      "30/30 [==============================] - 2s 68ms/step - loss: 1.3257 - accuracy: 0.4718 - val_loss: 1.4918 - val_accuracy: 0.4141\n",
      "Epoch 129/1009\n",
      "30/30 [==============================] - 2s 67ms/step - loss: 1.2927 - accuracy: 0.5214 - val_loss: 1.4984 - val_accuracy: 0.4258\n",
      "Epoch 130/1009\n",
      "30/30 [==============================] - 2s 76ms/step - loss: 1.3055 - accuracy: 0.5125 - val_loss: 1.5328 - val_accuracy: 0.4023\n",
      "Epoch 131/1009\n",
      "30/30 [==============================] - 2s 74ms/step - loss: 1.3222 - accuracy: 0.5120 - val_loss: 1.4817 - val_accuracy: 0.4258\n",
      "Epoch 132/1009\n",
      "30/30 [==============================] - 2s 72ms/step - loss: 1.2870 - accuracy: 0.5283 - val_loss: 1.4961 - val_accuracy: 0.4102\n",
      "Epoch 133/1009\n",
      "30/30 [==============================] - 2s 77ms/step - loss: 1.2946 - accuracy: 0.5215 - val_loss: 1.4937 - val_accuracy: 0.4453\n",
      "Epoch 134/1009\n",
      "30/30 [==============================] - 2s 77ms/step - loss: 1.3023 - accuracy: 0.5117 - val_loss: 1.4974 - val_accuracy: 0.4180\n",
      "Epoch 135/1009\n",
      "30/30 [==============================] - 2s 77ms/step - loss: 1.3358 - accuracy: 0.4856 - val_loss: 1.4792 - val_accuracy: 0.4492\n",
      "Epoch 136/1009\n",
      "30/30 [==============================] - 2s 77ms/step - loss: 1.3300 - accuracy: 0.5193 - val_loss: 1.4752 - val_accuracy: 0.4180\n",
      "Epoch 137/1009\n",
      "30/30 [==============================] - 2s 74ms/step - loss: 1.3155 - accuracy: 0.4932 - val_loss: 1.4784 - val_accuracy: 0.4297\n",
      "Epoch 138/1009\n",
      "30/30 [==============================] - 2s 72ms/step - loss: 1.3011 - accuracy: 0.4966 - val_loss: 1.4730 - val_accuracy: 0.4180\n",
      "Epoch 139/1009\n",
      "30/30 [==============================] - 2s 78ms/step - loss: 1.2741 - accuracy: 0.5353 - val_loss: 1.4685 - val_accuracy: 0.4219\n",
      "Epoch 140/1009\n",
      "30/30 [==============================] - 2s 70ms/step - loss: 1.3277 - accuracy: 0.4989 - val_loss: 1.4861 - val_accuracy: 0.4102\n",
      "Epoch 141/1009\n",
      "30/30 [==============================] - 2s 76ms/step - loss: 1.3121 - accuracy: 0.4895 - val_loss: 1.5163 - val_accuracy: 0.4180\n",
      "Epoch 142/1009\n",
      "30/30 [==============================] - 2s 76ms/step - loss: 1.3235 - accuracy: 0.4964 - val_loss: 1.4843 - val_accuracy: 0.4141\n",
      "Epoch 143/1009\n",
      "30/30 [==============================] - 2s 82ms/step - loss: 1.2696 - accuracy: 0.5411 - val_loss: 1.4725 - val_accuracy: 0.4258\n",
      "Epoch 144/1009\n",
      "30/30 [==============================] - 2s 75ms/step - loss: 1.2753 - accuracy: 0.5333 - val_loss: 1.4661 - val_accuracy: 0.4492\n",
      "Epoch 145/1009\n",
      "30/30 [==============================] - 2s 69ms/step - loss: 1.3018 - accuracy: 0.5026 - val_loss: 1.4991 - val_accuracy: 0.4141\n",
      "Epoch 146/1009\n",
      "30/30 [==============================] - 2s 70ms/step - loss: 1.2742 - accuracy: 0.5153 - val_loss: 1.4607 - val_accuracy: 0.4570\n",
      "Epoch 147/1009\n",
      "30/30 [==============================] - 2s 67ms/step - loss: 1.2433 - accuracy: 0.5383 - val_loss: 1.4682 - val_accuracy: 0.4219\n",
      "Epoch 148/1009\n",
      "30/30 [==============================] - 2s 67ms/step - loss: 1.3096 - accuracy: 0.4982 - val_loss: 1.4635 - val_accuracy: 0.4219\n",
      "Epoch 149/1009\n",
      "30/30 [==============================] - 2s 73ms/step - loss: 1.2387 - accuracy: 0.5765 - val_loss: 1.4657 - val_accuracy: 0.4180\n",
      "Epoch 150/1009\n",
      "30/30 [==============================] - 2s 72ms/step - loss: 1.2970 - accuracy: 0.5256 - val_loss: 1.4631 - val_accuracy: 0.4336\n",
      "Epoch 151/1009\n",
      "30/30 [==============================] - 3s 89ms/step - loss: 1.2606 - accuracy: 0.5214 - val_loss: 1.4554 - val_accuracy: 0.4375\n",
      "Epoch 152/1009\n",
      "30/30 [==============================] - 2s 83ms/step - loss: 1.2478 - accuracy: 0.5227 - val_loss: 1.4794 - val_accuracy: 0.4336\n",
      "Epoch 153/1009\n",
      "30/30 [==============================] - 2s 74ms/step - loss: 1.1954 - accuracy: 0.5689 - val_loss: 1.4833 - val_accuracy: 0.4375\n",
      "Epoch 154/1009\n",
      "30/30 [==============================] - 2s 78ms/step - loss: 1.1923 - accuracy: 0.5727 - val_loss: 1.4573 - val_accuracy: 0.4297\n",
      "Epoch 155/1009\n",
      "30/30 [==============================] - 2s 72ms/step - loss: 1.2732 - accuracy: 0.5072 - val_loss: 1.4587 - val_accuracy: 0.4258\n",
      "Epoch 156/1009\n",
      "30/30 [==============================] - 2s 75ms/step - loss: 1.2095 - accuracy: 0.5549 - val_loss: 1.4866 - val_accuracy: 0.4375\n",
      "Epoch 157/1009\n",
      "30/30 [==============================] - 2s 74ms/step - loss: 1.2597 - accuracy: 0.5351 - val_loss: 1.4662 - val_accuracy: 0.4453\n",
      "Epoch 158/1009\n",
      "30/30 [==============================] - 2s 76ms/step - loss: 1.2840 - accuracy: 0.5070 - val_loss: 1.4738 - val_accuracy: 0.4258\n",
      "Epoch 159/1009\n",
      "30/30 [==============================] - 2s 71ms/step - loss: 1.1918 - accuracy: 0.5696 - val_loss: 1.4798 - val_accuracy: 0.4141\n",
      "Epoch 160/1009\n",
      "30/30 [==============================] - 2s 67ms/step - loss: 1.2601 - accuracy: 0.5287 - val_loss: 1.4610 - val_accuracy: 0.4336\n",
      "Epoch 161/1009\n",
      "30/30 [==============================] - 2s 71ms/step - loss: 1.2413 - accuracy: 0.5141 - val_loss: 1.4833 - val_accuracy: 0.4180\n",
      "Epoch 162/1009\n",
      "30/30 [==============================] - 2s 66ms/step - loss: 1.2345 - accuracy: 0.5245 - val_loss: 1.4715 - val_accuracy: 0.4297\n",
      "Epoch 163/1009\n",
      "30/30 [==============================] - 2s 69ms/step - loss: 1.2636 - accuracy: 0.5339 - val_loss: 1.4474 - val_accuracy: 0.4219\n",
      "Epoch 164/1009\n",
      "30/30 [==============================] - 2s 70ms/step - loss: 1.1950 - accuracy: 0.5824 - val_loss: 1.4668 - val_accuracy: 0.4297\n",
      "Epoch 165/1009\n",
      "30/30 [==============================] - 2s 67ms/step - loss: 1.2107 - accuracy: 0.5698 - val_loss: 1.4643 - val_accuracy: 0.4570\n",
      "Epoch 166/1009\n",
      "30/30 [==============================] - 2s 66ms/step - loss: 1.2693 - accuracy: 0.5016 - val_loss: 1.4782 - val_accuracy: 0.4453\n",
      "Epoch 167/1009\n",
      "30/30 [==============================] - 2s 67ms/step - loss: 1.2433 - accuracy: 0.5064 - val_loss: 1.4568 - val_accuracy: 0.4375\n",
      "Epoch 168/1009\n",
      "30/30 [==============================] - 2s 69ms/step - loss: 1.1901 - accuracy: 0.5329 - val_loss: 1.4703 - val_accuracy: 0.4180\n",
      "Epoch 169/1009\n",
      "30/30 [==============================] - 2s 66ms/step - loss: 1.2412 - accuracy: 0.5369 - val_loss: 1.4834 - val_accuracy: 0.4102\n",
      "Epoch 170/1009\n",
      "30/30 [==============================] - 2s 67ms/step - loss: 1.2126 - accuracy: 0.5473 - val_loss: 1.5251 - val_accuracy: 0.4180\n",
      "Epoch 171/1009\n",
      "30/30 [==============================] - 2s 66ms/step - loss: 1.2683 - accuracy: 0.5358 - val_loss: 1.4433 - val_accuracy: 0.4375\n",
      "Epoch 172/1009\n",
      "30/30 [==============================] - 2s 68ms/step - loss: 1.2163 - accuracy: 0.5688 - val_loss: 1.4529 - val_accuracy: 0.4141\n",
      "Epoch 173/1009\n",
      "30/30 [==============================] - 2s 75ms/step - loss: 1.1998 - accuracy: 0.5437 - val_loss: 1.4490 - val_accuracy: 0.4492\n",
      "Epoch 174/1009\n",
      "30/30 [==============================] - 2s 70ms/step - loss: 1.2258 - accuracy: 0.5213 - val_loss: 1.4617 - val_accuracy: 0.4141\n",
      "Epoch 175/1009\n",
      "30/30 [==============================] - 2s 80ms/step - loss: 1.1725 - accuracy: 0.5856 - val_loss: 1.4432 - val_accuracy: 0.4531\n",
      "Epoch 176/1009\n",
      "30/30 [==============================] - 2s 76ms/step - loss: 1.1907 - accuracy: 0.5741 - val_loss: 1.4595 - val_accuracy: 0.4336\n",
      "Epoch 177/1009\n",
      "30/30 [==============================] - 2s 73ms/step - loss: 1.1991 - accuracy: 0.5570 - val_loss: 1.4434 - val_accuracy: 0.4453\n",
      "Epoch 178/1009\n",
      "30/30 [==============================] - 2s 75ms/step - loss: 1.2109 - accuracy: 0.5506 - val_loss: 1.4597 - val_accuracy: 0.4258\n",
      "Epoch 179/1009\n",
      "30/30 [==============================] - 2s 70ms/step - loss: 1.1284 - accuracy: 0.5971 - val_loss: 1.5061 - val_accuracy: 0.4102\n",
      "Epoch 180/1009\n",
      "30/30 [==============================] - 2s 70ms/step - loss: 1.1616 - accuracy: 0.5714 - val_loss: 1.4623 - val_accuracy: 0.4219\n",
      "Epoch 181/1009\n",
      "30/30 [==============================] - 2s 69ms/step - loss: 1.2043 - accuracy: 0.5453 - val_loss: 1.4362 - val_accuracy: 0.4375\n",
      "Epoch 182/1009\n",
      "30/30 [==============================] - 2s 70ms/step - loss: 1.1745 - accuracy: 0.5678 - val_loss: 1.4491 - val_accuracy: 0.4570\n",
      "Epoch 183/1009\n",
      "30/30 [==============================] - 2s 73ms/step - loss: 1.1548 - accuracy: 0.5869 - val_loss: 1.4426 - val_accuracy: 0.4375\n",
      "Epoch 184/1009\n",
      "30/30 [==============================] - 2s 72ms/step - loss: 1.1841 - accuracy: 0.5888 - val_loss: 1.4318 - val_accuracy: 0.4570\n",
      "Epoch 185/1009\n",
      "30/30 [==============================] - 2s 73ms/step - loss: 1.1554 - accuracy: 0.5874 - val_loss: 1.4837 - val_accuracy: 0.4414\n",
      "Epoch 186/1009\n",
      "30/30 [==============================] - 2s 75ms/step - loss: 1.1475 - accuracy: 0.5979 - val_loss: 1.4543 - val_accuracy: 0.4453\n",
      "Epoch 187/1009\n",
      "30/30 [==============================] - 2s 80ms/step - loss: 1.1665 - accuracy: 0.5577 - val_loss: 1.4573 - val_accuracy: 0.4375\n",
      "Epoch 188/1009\n",
      "30/30 [==============================] - 2s 81ms/step - loss: 1.1891 - accuracy: 0.5467 - val_loss: 1.5164 - val_accuracy: 0.4375\n",
      "Epoch 189/1009\n",
      "30/30 [==============================] - 2s 78ms/step - loss: 1.1839 - accuracy: 0.5868 - val_loss: 1.4387 - val_accuracy: 0.4414\n",
      "Epoch 190/1009\n",
      "30/30 [==============================] - 3s 84ms/step - loss: 1.1987 - accuracy: 0.5801 - val_loss: 1.4251 - val_accuracy: 0.4492\n",
      "Epoch 191/1009\n",
      "30/30 [==============================] - 2s 84ms/step - loss: 1.1492 - accuracy: 0.5721 - val_loss: 1.4402 - val_accuracy: 0.4336\n",
      "Epoch 192/1009\n",
      "30/30 [==============================] - 3s 84ms/step - loss: 1.1617 - accuracy: 0.5775 - val_loss: 1.4401 - val_accuracy: 0.4414\n",
      "Epoch 193/1009\n",
      "30/30 [==============================] - 2s 83ms/step - loss: 1.1308 - accuracy: 0.5599 - val_loss: 1.4303 - val_accuracy: 0.4414\n",
      "Epoch 194/1009\n",
      "30/30 [==============================] - 2s 83ms/step - loss: 1.1398 - accuracy: 0.5601 - val_loss: 1.4312 - val_accuracy: 0.4609\n",
      "Epoch 195/1009\n",
      "30/30 [==============================] - 2s 76ms/step - loss: 1.1535 - accuracy: 0.5750 - val_loss: 1.4287 - val_accuracy: 0.4570\n",
      "Epoch 196/1009\n",
      "30/30 [==============================] - 2s 71ms/step - loss: 1.1475 - accuracy: 0.5969 - val_loss: 1.4527 - val_accuracy: 0.4492\n",
      "Epoch 197/1009\n",
      "30/30 [==============================] - 2s 73ms/step - loss: 1.1933 - accuracy: 0.5468 - val_loss: 1.4428 - val_accuracy: 0.4336\n",
      "Epoch 198/1009\n",
      "30/30 [==============================] - 2s 69ms/step - loss: 1.1314 - accuracy: 0.5916 - val_loss: 1.4246 - val_accuracy: 0.4492\n",
      "Epoch 199/1009\n",
      "30/30 [==============================] - 2s 71ms/step - loss: 1.1093 - accuracy: 0.5958 - val_loss: 1.4207 - val_accuracy: 0.4648\n",
      "Epoch 200/1009\n",
      "30/30 [==============================] - 3s 89ms/step - loss: 1.1198 - accuracy: 0.5912 - val_loss: 1.4305 - val_accuracy: 0.4453\n",
      "Epoch 201/1009\n",
      "30/30 [==============================] - 2s 69ms/step - loss: 1.1289 - accuracy: 0.5983 - val_loss: 1.4688 - val_accuracy: 0.4180\n",
      "Epoch 202/1009\n",
      "30/30 [==============================] - 2s 71ms/step - loss: 1.1488 - accuracy: 0.5486 - val_loss: 1.4593 - val_accuracy: 0.4375\n",
      "Epoch 203/1009\n",
      "30/30 [==============================] - 2s 68ms/step - loss: 1.1722 - accuracy: 0.5644 - val_loss: 1.4397 - val_accuracy: 0.4609\n",
      "Epoch 204/1009\n",
      "30/30 [==============================] - 2s 71ms/step - loss: 1.1591 - accuracy: 0.5748 - val_loss: 1.4499 - val_accuracy: 0.4375\n",
      "Epoch 205/1009\n",
      "30/30 [==============================] - 2s 73ms/step - loss: 1.0999 - accuracy: 0.5890 - val_loss: 1.4223 - val_accuracy: 0.4609\n",
      "Epoch 206/1009\n",
      "30/30 [==============================] - 2s 71ms/step - loss: 1.1664 - accuracy: 0.5675 - val_loss: 1.4301 - val_accuracy: 0.4570\n",
      "Epoch 207/1009\n",
      "30/30 [==============================] - 2s 72ms/step - loss: 1.1567 - accuracy: 0.5984 - val_loss: 1.4379 - val_accuracy: 0.4492\n",
      "Epoch 208/1009\n",
      "30/30 [==============================] - 2s 68ms/step - loss: 1.1787 - accuracy: 0.5567 - val_loss: 1.4302 - val_accuracy: 0.4570\n",
      "Epoch 209/1009\n",
      "30/30 [==============================] - 2s 73ms/step - loss: 1.0992 - accuracy: 0.5945 - val_loss: 1.4310 - val_accuracy: 0.4453\n",
      "Epoch 210/1009\n",
      "30/30 [==============================] - 2s 68ms/step - loss: 1.1310 - accuracy: 0.5895 - val_loss: 1.4512 - val_accuracy: 0.4297\n",
      "Epoch 211/1009\n",
      "30/30 [==============================] - 2s 67ms/step - loss: 1.0989 - accuracy: 0.6132 - val_loss: 1.4747 - val_accuracy: 0.4258\n",
      "Epoch 212/1009\n",
      "30/30 [==============================] - 2s 67ms/step - loss: 1.0906 - accuracy: 0.6093 - val_loss: 1.4412 - val_accuracy: 0.4453\n",
      "Epoch 213/1009\n",
      "30/30 [==============================] - 2s 69ms/step - loss: 1.1414 - accuracy: 0.5995 - val_loss: 1.4342 - val_accuracy: 0.4531\n",
      "Epoch 214/1009\n",
      "30/30 [==============================] - 2s 67ms/step - loss: 1.0942 - accuracy: 0.5982 - val_loss: 1.4440 - val_accuracy: 0.4375\n",
      "Epoch 215/1009\n",
      "30/30 [==============================] - 2s 66ms/step - loss: 1.1018 - accuracy: 0.6102 - val_loss: 1.4812 - val_accuracy: 0.4062\n",
      "Epoch 216/1009\n",
      "30/30 [==============================] - 2s 76ms/step - loss: 1.1071 - accuracy: 0.5937 - val_loss: 1.4449 - val_accuracy: 0.4219\n",
      "Epoch 217/1009\n",
      "30/30 [==============================] - 3s 86ms/step - loss: 1.1005 - accuracy: 0.6226 - val_loss: 1.4237 - val_accuracy: 0.4570\n",
      "Epoch 218/1009\n",
      "30/30 [==============================] - 2s 75ms/step - loss: 1.0967 - accuracy: 0.6061 - val_loss: 1.4208 - val_accuracy: 0.4570\n",
      "Epoch 219/1009\n",
      "30/30 [==============================] - 2s 67ms/step - loss: 1.0954 - accuracy: 0.6012 - val_loss: 1.4508 - val_accuracy: 0.4414\n",
      "Epoch 220/1009\n",
      "30/30 [==============================] - 2s 68ms/step - loss: 1.1188 - accuracy: 0.5616 - val_loss: 1.4208 - val_accuracy: 0.4570\n",
      "Epoch 221/1009\n",
      "30/30 [==============================] - 2s 67ms/step - loss: 1.0612 - accuracy: 0.6149 - val_loss: 1.4337 - val_accuracy: 0.4336\n",
      "Epoch 222/1009\n",
      "30/30 [==============================] - 2s 68ms/step - loss: 1.0760 - accuracy: 0.6163 - val_loss: 1.4241 - val_accuracy: 0.4336\n",
      "Epoch 223/1009\n",
      "30/30 [==============================] - 2s 68ms/step - loss: 1.0959 - accuracy: 0.6087 - val_loss: 1.4388 - val_accuracy: 0.4453\n",
      "Epoch 224/1009\n",
      "30/30 [==============================] - 3s 84ms/step - loss: 1.0805 - accuracy: 0.5954 - val_loss: 1.4382 - val_accuracy: 0.4570\n",
      "Epoch 225/1009\n",
      "30/30 [==============================] - 2s 73ms/step - loss: 1.0936 - accuracy: 0.6008 - val_loss: 1.4341 - val_accuracy: 0.4492\n",
      "Epoch 226/1009\n",
      "30/30 [==============================] - 2s 75ms/step - loss: 1.0900 - accuracy: 0.6018 - val_loss: 1.4421 - val_accuracy: 0.4219\n",
      "Epoch 227/1009\n",
      "30/30 [==============================] - 2s 71ms/step - loss: 1.0825 - accuracy: 0.6126 - val_loss: 1.4442 - val_accuracy: 0.4492\n",
      "Epoch 228/1009\n",
      "30/30 [==============================] - 2s 73ms/step - loss: 1.0630 - accuracy: 0.6117 - val_loss: 1.4748 - val_accuracy: 0.4336\n",
      "Epoch 229/1009\n",
      "30/30 [==============================] - 2s 81ms/step - loss: 1.1083 - accuracy: 0.5909 - val_loss: 1.4225 - val_accuracy: 0.4336\n",
      "Epoch 230/1009\n",
      "30/30 [==============================] - 2s 75ms/step - loss: 1.0963 - accuracy: 0.6037 - val_loss: 1.4425 - val_accuracy: 0.4297\n",
      "Epoch 231/1009\n",
      "30/30 [==============================] - 2s 81ms/step - loss: 1.1016 - accuracy: 0.5933 - val_loss: 1.4864 - val_accuracy: 0.4219\n",
      "Epoch 232/1009\n",
      "30/30 [==============================] - 2s 84ms/step - loss: 1.0866 - accuracy: 0.5953 - val_loss: 1.4851 - val_accuracy: 0.4297\n",
      "Epoch 233/1009\n",
      "30/30 [==============================] - 2s 74ms/step - loss: 1.1027 - accuracy: 0.5890 - val_loss: 1.4354 - val_accuracy: 0.4414\n",
      "Epoch 234/1009\n",
      "30/30 [==============================] - 2s 72ms/step - loss: 1.0714 - accuracy: 0.5944 - val_loss: 1.4475 - val_accuracy: 0.4258\n",
      "Epoch 235/1009\n",
      "30/30 [==============================] - 2s 71ms/step - loss: 1.0638 - accuracy: 0.6271 - val_loss: 1.4326 - val_accuracy: 0.4492\n",
      "Epoch 236/1009\n",
      "30/30 [==============================] - 2s 80ms/step - loss: 1.0288 - accuracy: 0.6331 - val_loss: 1.4305 - val_accuracy: 0.4375\n",
      "Epoch 237/1009\n",
      "30/30 [==============================] - 3s 87ms/step - loss: 1.0711 - accuracy: 0.6184 - val_loss: 1.4925 - val_accuracy: 0.4258\n",
      "Epoch 238/1009\n",
      "30/30 [==============================] - 2s 84ms/step - loss: 1.0617 - accuracy: 0.6235 - val_loss: 1.4717 - val_accuracy: 0.4375\n",
      "Epoch 239/1009\n",
      "30/30 [==============================] - 2s 74ms/step - loss: 1.0566 - accuracy: 0.6029 - val_loss: 1.4496 - val_accuracy: 0.4336\n",
      "Epoch 240/1009\n",
      "30/30 [==============================] - 2s 74ms/step - loss: 1.0445 - accuracy: 0.6332 - val_loss: 1.5018 - val_accuracy: 0.4102\n",
      "Epoch 241/1009\n",
      "30/30 [==============================] - 2s 70ms/step - loss: 1.0786 - accuracy: 0.6140 - val_loss: 1.4436 - val_accuracy: 0.4453\n",
      "Epoch 242/1009\n",
      "30/30 [==============================] - 2s 68ms/step - loss: 1.0589 - accuracy: 0.6038 - val_loss: 1.4747 - val_accuracy: 0.4336\n",
      "Epoch 243/1009\n",
      "30/30 [==============================] - 2s 67ms/step - loss: 1.1149 - accuracy: 0.5920 - val_loss: 1.4427 - val_accuracy: 0.4258\n",
      "Epoch 244/1009\n",
      "30/30 [==============================] - 2s 67ms/step - loss: 1.0268 - accuracy: 0.6338 - val_loss: 1.4321 - val_accuracy: 0.4453\n",
      "Epoch 245/1009\n",
      "30/30 [==============================] - 2s 71ms/step - loss: 1.0260 - accuracy: 0.6181 - val_loss: 1.4313 - val_accuracy: 0.4336\n",
      "Epoch 246/1009\n",
      "30/30 [==============================] - 2s 69ms/step - loss: 1.0578 - accuracy: 0.6174 - val_loss: 1.4225 - val_accuracy: 0.4609\n",
      "Epoch 247/1009\n",
      "30/30 [==============================] - 2s 73ms/step - loss: 1.0599 - accuracy: 0.6117 - val_loss: 1.4285 - val_accuracy: 0.4336\n",
      "Epoch 248/1009\n",
      "30/30 [==============================] - 2s 68ms/step - loss: 1.0387 - accuracy: 0.6278 - val_loss: 1.4330 - val_accuracy: 0.4414\n",
      "Epoch 249/1009\n",
      "30/30 [==============================] - 2s 71ms/step - loss: 1.0624 - accuracy: 0.6221 - val_loss: 1.4875 - val_accuracy: 0.4102\n",
      "Epoch 250/1009\n",
      "30/30 [==============================] - 3s 84ms/step - loss: 1.0528 - accuracy: 0.6294 - val_loss: 1.4407 - val_accuracy: 0.4219\n",
      "Epoch 251/1009\n",
      "30/30 [==============================] - 3s 84ms/step - loss: 1.0171 - accuracy: 0.6256 - val_loss: 1.4669 - val_accuracy: 0.4141\n",
      "Epoch 252/1009\n",
      "30/30 [==============================] - 2s 83ms/step - loss: 0.9975 - accuracy: 0.6252 - val_loss: 1.4384 - val_accuracy: 0.4531\n",
      "Epoch 253/1009\n",
      "30/30 [==============================] - 3s 88ms/step - loss: 1.0398 - accuracy: 0.6106 - val_loss: 1.4590 - val_accuracy: 0.4141\n",
      "Epoch 254/1009\n",
      "30/30 [==============================] - 3s 89ms/step - loss: 1.0021 - accuracy: 0.6352 - val_loss: 1.4481 - val_accuracy: 0.4219\n",
      "Epoch 255/1009\n",
      "30/30 [==============================] - 2s 77ms/step - loss: 1.0397 - accuracy: 0.6249 - val_loss: 1.4429 - val_accuracy: 0.4297\n",
      "Epoch 256/1009\n",
      "30/30 [==============================] - 2s 76ms/step - loss: 0.9883 - accuracy: 0.6372 - val_loss: 1.4308 - val_accuracy: 0.4414\n",
      "Epoch 257/1009\n",
      "30/30 [==============================] - 2s 80ms/step - loss: 0.9827 - accuracy: 0.6575 - val_loss: 1.4218 - val_accuracy: 0.4414\n",
      "Epoch 258/1009\n",
      "30/30 [==============================] - 2s 83ms/step - loss: 1.0150 - accuracy: 0.6399 - val_loss: 1.4288 - val_accuracy: 0.4336\n",
      "Epoch 259/1009\n",
      "30/30 [==============================] - 2s 81ms/step - loss: 0.9800 - accuracy: 0.6583 - val_loss: 1.4338 - val_accuracy: 0.4570\n",
      "Epoch 260/1009\n",
      "30/30 [==============================] - 2s 70ms/step - loss: 1.0337 - accuracy: 0.6185 - val_loss: 1.4390 - val_accuracy: 0.4375\n",
      "Epoch 261/1009\n",
      "30/30 [==============================] - 2s 75ms/step - loss: 1.0432 - accuracy: 0.6232 - val_loss: 1.4242 - val_accuracy: 0.4375\n",
      "Epoch 262/1009\n",
      "30/30 [==============================] - 2s 80ms/step - loss: 0.9862 - accuracy: 0.6406 - val_loss: 1.4374 - val_accuracy: 0.4375\n",
      "Epoch 263/1009\n",
      "30/30 [==============================] - 2s 72ms/step - loss: 1.0117 - accuracy: 0.6301 - val_loss: 1.4185 - val_accuracy: 0.4531\n",
      "Epoch 264/1009\n",
      "30/30 [==============================] - 3s 89ms/step - loss: 1.0681 - accuracy: 0.5882 - val_loss: 1.4426 - val_accuracy: 0.4414\n",
      "Epoch 265/1009\n",
      "30/30 [==============================] - 3s 86ms/step - loss: 1.0038 - accuracy: 0.6293 - val_loss: 1.4454 - val_accuracy: 0.4336\n",
      "Epoch 266/1009\n",
      "30/30 [==============================] - 2s 82ms/step - loss: 0.9977 - accuracy: 0.6267 - val_loss: 1.4677 - val_accuracy: 0.4219\n",
      "Epoch 267/1009\n",
      "30/30 [==============================] - 2s 74ms/step - loss: 1.0018 - accuracy: 0.6342 - val_loss: 1.4334 - val_accuracy: 0.4375\n",
      "Epoch 268/1009\n",
      "30/30 [==============================] - 2s 74ms/step - loss: 1.0167 - accuracy: 0.6211 - val_loss: 1.4229 - val_accuracy: 0.4727\n",
      "Epoch 269/1009\n",
      "30/30 [==============================] - 2s 72ms/step - loss: 1.0266 - accuracy: 0.6098 - val_loss: 1.4683 - val_accuracy: 0.4258\n",
      "Epoch 270/1009\n",
      "30/30 [==============================] - 2s 68ms/step - loss: 0.9999 - accuracy: 0.6365 - val_loss: 1.4350 - val_accuracy: 0.4297\n",
      "Epoch 271/1009\n",
      "30/30 [==============================] - 2s 76ms/step - loss: 0.9946 - accuracy: 0.6494 - val_loss: 1.5115 - val_accuracy: 0.4180\n",
      "Epoch 272/1009\n",
      "30/30 [==============================] - 2s 73ms/step - loss: 0.9785 - accuracy: 0.6373 - val_loss: 1.4413 - val_accuracy: 0.4258\n",
      "Epoch 273/1009\n",
      "30/30 [==============================] - 2s 82ms/step - loss: 0.9353 - accuracy: 0.6633 - val_loss: 1.4503 - val_accuracy: 0.4180\n",
      "Epoch 274/1009\n",
      "30/30 [==============================] - 2s 77ms/step - loss: 1.0119 - accuracy: 0.6163 - val_loss: 1.4602 - val_accuracy: 0.3984\n",
      "Epoch 275/1009\n",
      "30/30 [==============================] - 2s 68ms/step - loss: 0.9723 - accuracy: 0.6387 - val_loss: 1.4481 - val_accuracy: 0.4297\n",
      "Epoch 276/1009\n",
      "30/30 [==============================] - 2s 78ms/step - loss: 1.0079 - accuracy: 0.6328 - val_loss: 1.4246 - val_accuracy: 0.4375\n",
      "Epoch 277/1009\n",
      "30/30 [==============================] - 2s 69ms/step - loss: 0.9551 - accuracy: 0.6681 - val_loss: 1.4545 - val_accuracy: 0.4375\n",
      "Epoch 278/1009\n",
      "30/30 [==============================] - 2s 73ms/step - loss: 0.9295 - accuracy: 0.6966 - val_loss: 1.4977 - val_accuracy: 0.4141\n",
      "Epoch 279/1009\n",
      "30/30 [==============================] - 2s 74ms/step - loss: 0.9909 - accuracy: 0.6279 - val_loss: 1.4403 - val_accuracy: 0.4531\n",
      "Epoch 280/1009\n",
      "30/30 [==============================] - 2s 73ms/step - loss: 1.0158 - accuracy: 0.6446 - val_loss: 1.4525 - val_accuracy: 0.4453\n",
      "Epoch 281/1009\n",
      "30/30 [==============================] - 2s 79ms/step - loss: 1.0089 - accuracy: 0.6408 - val_loss: 1.4595 - val_accuracy: 0.4258\n",
      "Epoch 282/1009\n",
      "30/30 [==============================] - 2s 76ms/step - loss: 0.9823 - accuracy: 0.6388 - val_loss: 1.4848 - val_accuracy: 0.4180\n",
      "Epoch 283/1009\n",
      "30/30 [==============================] - 2s 74ms/step - loss: 0.9957 - accuracy: 0.6275 - val_loss: 1.4697 - val_accuracy: 0.4141\n",
      "Epoch 284/1009\n",
      "30/30 [==============================] - 2s 73ms/step - loss: 0.9630 - accuracy: 0.6507 - val_loss: 1.4578 - val_accuracy: 0.4414\n",
      "Epoch 285/1009\n",
      "30/30 [==============================] - 2s 74ms/step - loss: 0.9845 - accuracy: 0.6544 - val_loss: 1.4415 - val_accuracy: 0.4297\n",
      "Epoch 286/1009\n",
      "30/30 [==============================] - 2s 74ms/step - loss: 0.9859 - accuracy: 0.6282 - val_loss: 1.4427 - val_accuracy: 0.4336\n",
      "Epoch 287/1009\n",
      "30/30 [==============================] - 2s 67ms/step - loss: 0.9707 - accuracy: 0.6588 - val_loss: 1.4445 - val_accuracy: 0.4258\n",
      "Epoch 288/1009\n",
      "30/30 [==============================] - 2s 68ms/step - loss: 0.9708 - accuracy: 0.6701 - val_loss: 1.4475 - val_accuracy: 0.4492\n",
      "Epoch 289/1009\n",
      "30/30 [==============================] - 2s 67ms/step - loss: 0.9688 - accuracy: 0.6594 - val_loss: 1.5163 - val_accuracy: 0.4219\n",
      "Epoch 290/1009\n",
      "30/30 [==============================] - 2s 82ms/step - loss: 0.9561 - accuracy: 0.6572 - val_loss: 1.4225 - val_accuracy: 0.4297\n",
      "Epoch 291/1009\n",
      "30/30 [==============================] - 2s 74ms/step - loss: 0.9609 - accuracy: 0.6550 - val_loss: 1.4779 - val_accuracy: 0.4375\n",
      "Epoch 292/1009\n",
      "30/30 [==============================] - 2s 71ms/step - loss: 0.9458 - accuracy: 0.6572 - val_loss: 1.4368 - val_accuracy: 0.4453\n",
      "Epoch 293/1009\n",
      "30/30 [==============================] - 2s 75ms/step - loss: 0.9999 - accuracy: 0.6356 - val_loss: 1.4587 - val_accuracy: 0.4297\n",
      "Epoch 294/1009\n",
      "30/30 [==============================] - 2s 75ms/step - loss: 0.8975 - accuracy: 0.7053 - val_loss: 1.4457 - val_accuracy: 0.4375\n",
      "Epoch 295/1009\n",
      "30/30 [==============================] - 2s 84ms/step - loss: 0.9546 - accuracy: 0.6463 - val_loss: 1.5278 - val_accuracy: 0.4258\n",
      "Epoch 296/1009\n",
      "30/30 [==============================] - 2s 69ms/step - loss: 0.9193 - accuracy: 0.6850 - val_loss: 1.4353 - val_accuracy: 0.4414\n",
      "Epoch 297/1009\n",
      "30/30 [==============================] - 2s 71ms/step - loss: 0.9504 - accuracy: 0.6372 - val_loss: 1.4523 - val_accuracy: 0.4531\n",
      "Epoch 298/1009\n",
      "30/30 [==============================] - 2s 75ms/step - loss: 0.9499 - accuracy: 0.6689 - val_loss: 1.4659 - val_accuracy: 0.4297\n",
      "Epoch 299/1009\n",
      "30/30 [==============================] - 2s 71ms/step - loss: 0.9215 - accuracy: 0.6684 - val_loss: 1.4469 - val_accuracy: 0.4414\n",
      "Epoch 300/1009\n",
      "30/30 [==============================] - 2s 67ms/step - loss: 0.9082 - accuracy: 0.6832 - val_loss: 1.4310 - val_accuracy: 0.4531\n",
      "Epoch 301/1009\n",
      "30/30 [==============================] - 2s 69ms/step - loss: 0.9126 - accuracy: 0.6884 - val_loss: 1.4285 - val_accuracy: 0.4492\n",
      "Epoch 302/1009\n",
      "30/30 [==============================] - 2s 75ms/step - loss: 0.9545 - accuracy: 0.6630 - val_loss: 1.4344 - val_accuracy: 0.4609\n",
      "Epoch 303/1009\n",
      "30/30 [==============================] - 2s 76ms/step - loss: 0.9620 - accuracy: 0.6672 - val_loss: 1.4442 - val_accuracy: 0.4414\n",
      "Epoch 304/1009\n",
      "30/30 [==============================] - 2s 74ms/step - loss: 0.9319 - accuracy: 0.6775 - val_loss: 1.4654 - val_accuracy: 0.4336\n",
      "Epoch 305/1009\n",
      "30/30 [==============================] - 2s 68ms/step - loss: 0.9226 - accuracy: 0.6916 - val_loss: 1.4517 - val_accuracy: 0.4258\n",
      "Epoch 306/1009\n",
      "30/30 [==============================] - 3s 85ms/step - loss: 0.9303 - accuracy: 0.6714 - val_loss: 1.4387 - val_accuracy: 0.4531\n",
      "Epoch 307/1009\n",
      "30/30 [==============================] - 2s 74ms/step - loss: 0.9387 - accuracy: 0.6778 - val_loss: 1.4650 - val_accuracy: 0.4180\n",
      "Epoch 308/1009\n",
      "30/30 [==============================] - 2s 72ms/step - loss: 0.9311 - accuracy: 0.6738 - val_loss: 1.4458 - val_accuracy: 0.4336\n",
      "Epoch 309/1009\n",
      "30/30 [==============================] - 2s 69ms/step - loss: 0.9585 - accuracy: 0.6622 - val_loss: 1.4714 - val_accuracy: 0.4219\n",
      "Epoch 310/1009\n",
      "30/30 [==============================] - 2s 69ms/step - loss: 0.8981 - accuracy: 0.7043 - val_loss: 1.4547 - val_accuracy: 0.4375\n",
      "Epoch 311/1009\n",
      "30/30 [==============================] - 2s 67ms/step - loss: 0.9185 - accuracy: 0.6484 - val_loss: 1.4549 - val_accuracy: 0.4375\n",
      "Epoch 312/1009\n",
      "30/30 [==============================] - 2s 67ms/step - loss: 0.9594 - accuracy: 0.6557 - val_loss: 1.4374 - val_accuracy: 0.4570\n",
      "Epoch 313/1009\n",
      "30/30 [==============================] - 2s 66ms/step - loss: 0.9195 - accuracy: 0.6911 - val_loss: 1.4373 - val_accuracy: 0.4492\n",
      "Epoch 314/1009\n",
      "30/30 [==============================] - 2s 73ms/step - loss: 0.9181 - accuracy: 0.6722 - val_loss: 1.4282 - val_accuracy: 0.4570\n",
      "Epoch 315/1009\n",
      "30/30 [==============================] - 2s 72ms/step - loss: 0.9186 - accuracy: 0.6801 - val_loss: 1.4479 - val_accuracy: 0.4336\n",
      "Epoch 316/1009\n",
      "30/30 [==============================] - 2s 68ms/step - loss: 0.9153 - accuracy: 0.6862 - val_loss: 1.4400 - val_accuracy: 0.4414\n",
      "Epoch 317/1009\n",
      "30/30 [==============================] - 2s 73ms/step - loss: 0.9458 - accuracy: 0.6498 - val_loss: 1.4416 - val_accuracy: 0.4688\n",
      "Epoch 318/1009\n",
      "30/30 [==============================] - 2s 70ms/step - loss: 0.8589 - accuracy: 0.7037 - val_loss: 1.4403 - val_accuracy: 0.4531\n",
      "Epoch 319/1009\n",
      "30/30 [==============================] - 2s 73ms/step - loss: 0.8966 - accuracy: 0.6839 - val_loss: 1.4420 - val_accuracy: 0.4609\n",
      "Epoch 320/1009\n",
      "30/30 [==============================] - 2s 72ms/step - loss: 0.9102 - accuracy: 0.6941 - val_loss: 1.4326 - val_accuracy: 0.4414\n",
      "Epoch 321/1009\n",
      "30/30 [==============================] - 2s 69ms/step - loss: 0.9149 - accuracy: 0.6967 - val_loss: 1.4657 - val_accuracy: 0.4414\n",
      "Epoch 322/1009\n",
      "30/30 [==============================] - 2s 68ms/step - loss: 0.9121 - accuracy: 0.6704 - val_loss: 1.4476 - val_accuracy: 0.4336\n",
      "Epoch 323/1009\n",
      "30/30 [==============================] - 2s 70ms/step - loss: 0.8994 - accuracy: 0.6844 - val_loss: 1.4898 - val_accuracy: 0.4141\n",
      "Epoch 324/1009\n",
      "30/30 [==============================] - 2s 69ms/step - loss: 0.9215 - accuracy: 0.6559 - val_loss: 1.4458 - val_accuracy: 0.4453\n",
      "Epoch 325/1009\n",
      "30/30 [==============================] - 2s 67ms/step - loss: 0.8648 - accuracy: 0.7091 - val_loss: 1.4531 - val_accuracy: 0.4453\n",
      "Epoch 326/1009\n",
      "30/30 [==============================] - 2s 78ms/step - loss: 0.9430 - accuracy: 0.6868 - val_loss: 1.4500 - val_accuracy: 0.4414\n",
      "Epoch 327/1009\n",
      "30/30 [==============================] - 2s 77ms/step - loss: 0.9063 - accuracy: 0.6893 - val_loss: 1.4528 - val_accuracy: 0.4297\n",
      "Epoch 328/1009\n",
      "30/30 [==============================] - 3s 84ms/step - loss: 0.9300 - accuracy: 0.6760 - val_loss: 1.4701 - val_accuracy: 0.4414\n",
      "Epoch 329/1009\n",
      "30/30 [==============================] - 2s 72ms/step - loss: 0.9142 - accuracy: 0.6748 - val_loss: 1.5161 - val_accuracy: 0.4375\n",
      "Epoch 330/1009\n",
      "30/30 [==============================] - 2s 77ms/step - loss: 0.9073 - accuracy: 0.6998 - val_loss: 1.5193 - val_accuracy: 0.4102\n",
      "Epoch 331/1009\n",
      "30/30 [==============================] - 2s 74ms/step - loss: 0.9192 - accuracy: 0.6662 - val_loss: 1.5008 - val_accuracy: 0.4180\n",
      "Epoch 332/1009\n",
      "30/30 [==============================] - 2s 76ms/step - loss: 0.8638 - accuracy: 0.6910 - val_loss: 1.4477 - val_accuracy: 0.4492\n",
      "Epoch 333/1009\n",
      "16/30 [===============>..............] - ETA: 1s - loss: 0.8956 - accuracy: 0.6740"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-41-09cd259a9caa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcnnhistory\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_traincnn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1009\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_testcnn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Miniconda3\\envs\\med4\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1098\u001b[0m                 _r=1):\n\u001b[0;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1100\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1101\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\med4\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 828\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"xla\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\med4\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    853\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    854\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 855\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    856\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    857\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\med4\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[1;32m-> 2943\u001b[1;33m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[0;32m   2944\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2945\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\med4\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1917\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1919\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\med4\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 560\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    561\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\med4\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 60\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cnnhistory=model.fit(x_traincnn, y_train, batch_size=32, epochs=1009, validation_data=(x_testcnn, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the accuracy and loss graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sigmoid\n",
    "plt.plot(cnnhistory.history['accuracy'])\n",
    "plt.plot(cnnhistory.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(cnnhistory.history['loss'])\n",
    "plt.plot(cnnhistory.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'Emotion_Voice_Detection_Model.h5'\n",
    "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
    "# Save model and weights\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "model.save(model_path)\n",
    "print('Saved trained model at %s ' % model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting emotions on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(x_testcnn, \n",
    "                         batch_size=32, \n",
    "                         verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds1=preds.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "abc = preds1.astype(int).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = (lb.inverse_transform((abc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preddf = pd.DataFrame({'predictedvalues': predictions})\n",
    "preddf[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "actual=y_test.argmax(axis=1)\n",
    "abc123 = actual.astype(int).flatten()\n",
    "actualvalues = (lb.inverse_transform((abc123)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actualdf = pd.DataFrame({'actualvalues': actualvalues})\n",
    "actualdf[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "finaldf = actualdf.join(preddf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actual v/s Predicted emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finaldf[170:180]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finaldf.groupby('actualvalues').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finaldf.groupby('predictedvalues').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "finaldf.to_csv('Predictions.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e9f2d4daf75730007b5d0e6679b928fcc70751f4ffc128675decc195714a3e13"
  },
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('med4': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
